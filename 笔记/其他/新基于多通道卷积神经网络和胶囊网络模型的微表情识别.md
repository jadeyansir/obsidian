#### ![](https://cdn.nlark.com/yuque/0/2022/jpeg/22897659/1670997505184-be50b9cc-e512-43ea-af8a-8d9c99ca2fd4.jpeg#averageHue=%23f7fbf8&crop=0&crop=0&crop=1&crop=1&id=AxiN8&originHeight=256&originWidth=272&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)单位代码 	10635	

学	号  112018333031421 



![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997505762-6684bfaa-c282-43fc-8c99-166f2754c89f.png#averageHue=%23c9c9c9&crop=0&crop=0&crop=1&crop=1&id=j15Rr&originHeight=195&originWidth=702&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
专业学位硕士论文
## 基于多通道卷积神经网络和胶囊网络模型的微表情识别

论文作者：刘念
指导教师：陈通 教授 专业学位类别：工程硕士
专业领域：电子与通信工程
提交论文日期：2021 年 4 月 28 日
论文答辩日期：2021 年 5 月 25 日学位授予单位：西南大学



中 国  重 庆
2021 年 5 月

# Micro-expression Recognition Based on Multi-Stream Convolutional Neural Network and Capsule Network Model


![](https://cdn.nlark.com/yuque/0/2022/jpeg/22897659/1670997506257-007be83d-3d01-46ec-af86-600aee929d2d.jpeg#averageHue=%23f7fbf8&crop=0&crop=0&crop=1&crop=1&id=eRdU2&originHeight=256&originWidth=272&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)



A Thesis Submitted to Southwest University in Partial Fulfillment of the Requirement for the Master’s Degree of Engineering
**By Liu Nian**

**Supervised by Prof.Chen Tong**

**Specialty: Electronics and Communication Engineering**


### College of Electronic and Information Engineering of Southwest University，Chongqing，China May，2021



目	录
[摘 要	I](#_bookmark0)
[Abstract	III](#_bookmark1)
[第一章 引言	1](#_bookmark2)

   1. [研究背景及意义	1](#_bookmark3)
   2. [国内外研究现状	3](#_bookmark4)
   3. [论文研究内容及创新点	7](#_bookmark5)
      1. [论文的研究内容	7](#_bookmark6)
      2. [论文的创新点	8](#_bookmark7)
   4. [论文的结构安排	8](#_bookmark8)

[第二章 自发微表情数据库及其预处理	11](#_bookmark9)

   1. [自发微表情数据库	11](#_bookmark10)
   2. [数据库预处理	12](#_bookmark11)
      1. [人脸关键点检测及裁剪	13](#_bookmark12)
      2. [人脸图像配准及尺寸归一化	14](#_bookmark13)
      3. [计算光流	15](#_bookmark14)

[第三章 五通道卷积神经网络与胶囊网络的建立	19](#_bookmark15)

   1. [深度学习	19](#_bookmark16)
      1. [神经元与感知器	19](#_bookmark17)
      2. [BP 神经网络	20](#_bookmark18)
      3. [深度神经网络	23](#_bookmark19)
   2. [卷积神经网络的基本原理	26](#_bookmark20)
      1. [局部感知与权值共享	26](#_bookmark21)
      2. [卷积层与池化层	27](#_bookmark22)
      3. [Softmax 分类	30](#_bookmark23)
   3. [胶囊网络	31](#_bookmark24)
      1. [胶囊网络的总体结构	31](#_bookmark25)
      2. [胶囊与神经元的区别	32](#_bookmark26)
      3. [胶囊网络中的姿态矩阵	33](#_bookmark27)
      4. [动态路由算法	34](#_bookmark28)
   4. [五通道卷积神经网络和胶囊网络模型的构建	35](#_bookmark29)
      1. [五通道卷积神经网络	35](#_bookmark30)
      2. [五通道特征融合	36](#_bookmark31)
      3. [五通道卷积神经网络和胶囊网络模型	37](#_bookmark32)

[第四章 实验测试与结果分析	39](#_bookmark33)

   1. [实验设置	39](#_bookmark34)
   2. [实验结果及分析	39](#_bookmark35)
      1. [顶帧作为输入对比有无胶囊网络	40](#_bookmark36)
      2. [顶帧结合开始帧或结束帧在卷积神经网络中的比较	41](#_bookmark37)
      3. [五通道卷积神经网络与胶囊网络模型和对比算法的识别结果比较42](#_bookmark38)[第五章 总结与展望	45](#_bookmark39)
   3. [论文总结	45](#_bookmark40)
   4. [论文展望	45](#_bookmark41)

[参考文献	47](#_bookmark42)
[致	谢	55](#_bookmark140)
[攻读硕士期间已发表的学术论文	57](#_bookmark141)
[攻读硕士期间参加的科研项目	59](#_bookmark142)

摘要
## 基于多通道卷积神经网络和胶囊网络模型的微表情识别
#### 学科专业：电子与通信工程 研究方向：情感计算与智能信息处理指导教师：陈通教授	作 者：刘念

## 摘  要
人脸面部表情作为内心情感的直观表现方式，在人们的日常生活、交流中扮演着重要的角色。根据面部肌肉运动幅度大小，表情通常分为宏表情和微表情。其中宏表情持续时间长、幅度大，易于发现和识别；而微表情持续时间短、幅度小，且微表情的产生不受人们主观意识的控制，其不可掩饰的特点能较好的揭示人们的内心情感。因此，通过识别微表情对准确分析人类的情感有着重要意义。微表情产生于无意识下的肌肉自发运动，随后迅速被大脑抑制而结束，该过
程的运动变化极其微小，通过肉眼捕捉、识别微表情是一项极具挑战的任务。在现有基于视觉的微表情识别方案中，主要利用微表情序列中的开始帧和顶帧来提取微表情特征信息，忽略了微表情消逝过程的影响，如微表情结束帧信息。而微表情消逝阶段可看作产生阶段的逆过程，理论上也包含大量有利于微表情识别的特征信息。
针对上述问题，论文提出了一种基于多通道卷积神经网络和胶囊网络模型
（CNNCapsNet）的微表情识别方法。首先，通过关注微表情产生到消逝整个变化过程，提出五个通道的数据输入（开始帧到顶帧的水平光流图和垂直光流图、顶帧到结束帧的水平光流图和垂直光流图，顶帧灰度图），利用只关注运动变化的光流降低无效特征对微表情识别结果的影响。接着，提出支持五通道输入的卷积神经网络结构，训练并作为特征提取描述器。为了进一步精炼特征信息，所提方法将五通道提取的特征融合后，再输入至胶囊网络模型，该模型能有效增强该特征信息，加强特征之间的关联性，提升微表情识别性能。最后在 CASME Ⅱ和 SAMM 数据库上对论文提出的方法进行了验证，其识别率分别为 64.63%和 55.88%，结果表明论文提出的方法能有效提升微表情的识别结果。

关键词：微表情识别；五通道卷积神经网络；胶囊网络模型；特征增强

Abstract
# Micro-expression Recognition Based on Multi-Stream Convolutional Neural Network and Capsule Network Model
Major: Electronics and Communication Engineering

Direction: Affective Computing and Intelligent Information Processing Supervisor: Prof.Chen Tong
Author: Liu Nian


# Abstract
Facial expressions as the visual expression of inner emotions, play an important role in people's daily life and communication. Expressions can be divided into macro and micro-expressions depending on the magnitude of facial muscle movement. Micro-expressions have long duration and large amplitude, and hence are easily detected and recognized, whereas micro-expressions are short duration and small magnitude, and generally cannot be consciously controlled. The unconcealable characteristics of micro-expressions can offen better reveal people's inner emotions. Therefore, it is important to accurately analyze human emotions by recognizing micro-expressions.
Micro-expressions arise from unconscious spontaneous muscle movements, which are then quickly ended by brain inhibition. Motion changes in this process are extremely small, hence capturing and recognizing micro-expressions using the naked eye is challenging. Current vision based micro-expression recognition schemes mainly use onset and apex frames in the micro-expression sequence to extract micro-expression feature information, and commonly neglect influences from micro-expression fading, i.e., offset frame information. Micro-expression fading can be regarded as the inverse to generation, and contains a much feature information beneficial to micro-expression recognition.
This thesis proposes a micro-expression recognition method combining a multi-stream convolutional neural network and capsule network model (CNNCapsNet) to address these problems. Considering the whole change process from micro-expression generation to fading allowed five data streams to be extracted as input (horizontal and vertical optical flow maps from onset to apex frame, horizontal and vertical optical flow maps from apex to offset frame, and grayscale map of the apex frame). Invalid feature effects on micro-expression recognition are reduced using optical flow, focusing only on motion changes. The proposed five input channel convolutional neural network structure was trained as a feature extraction descriptor, and extracted channel features were fused to further refine feature information, then input to the capsule network model to effectively enhance feature information and strengthen correlations between features to improve the micro-expression recognition performance. Finally, the proposed method was validated on CASME II and SAMM databases, achieving 64.63% and 55.88% recognition rate, respectively. The results show that proposed method can effectively improve the performance of micro-expressions recognition.

**Keywords:	Micro-expression	Recognition;	Five-stream Convolution	Neural Network; Capsule Network Model; Feature Enhancement**

第一章 引言

### 第一章 引言
#### 0.1 研究背景及意义

早期研究人员多采用基于手工特征的方法来识别微表情，通过设计手工特征来描述面部图像指定的特征，并使用传统机器学习模型进行分类。研究人员根据经验设计一些能够描述面部的边缘、纹理、面部运动等信息的手工特征来区分微表情。基于手工提取特征的主流方法可以归结为基于局部二值模式(Local Binary Pattern, LBP)的方法、基于光流的方法和基于变换域的方法。基于手工特征的方法虽然在描述微表情某些指定信息时具有很好的效果，但难以提取完整的微表情特征，从而限制了其识别能力。
随着深度学习技术的兴起，基于深度学习的微表情识别方法也取得了较大的进展。研究人员利用卷积网络(Convolution Neural Network, CNN)对图像特征的强大描述能力来提取更为全面的图像特征。然而，微表情样本量较少，且它们之间的差异并不明显，这些数据集的缺陷限制了神经网络的拟合能力。为了解决上述问题，研究人员从强化特征以及增加对具有更高可区分度的特征的学习权重等角度提出了许多微表情识别方法。然而，这些方法往往忽视了面部产生微表情时，各个关键的局部区域间所具有的联动关系以及它们在时域上发生变化的趋势对情感表达所做出的贡献。同时，大量噪声数据对实验结果的造成了影响，在一定程度上限制了微表情的识别准确率。
因为面部微表情是一种重要的非语言信号，它能够隐藏人们的真实情感，揭示人们真实的心理状态，为识别谎言提供有用的线索，可帮助人类进行辅助判断［11］，也可识别人类内心真实的情感，己被心理学、社会学、神经科学、计算机视觉等各个学科所探索。微表情在司法审讯［11］、公共安全管理［12］、心理咨询［13］等领域有重要的应用。在国内，中科院心理研究对微表情进行系统的研究；在国外，很多国家利用微表情进行反恐检测。
为了更好地将微观表征应用到真实场景中，专门研究微观表征的研究人员研究了微观表现的基本理论知识。1977年，Ekman等人分析和研究了人类面部肌肉运动。基于运动与面部表情之间的关系，建立了面部运动编码系统（FACS）[17]。该系统基于面部肌肉各部分的运动特点各不相同，面部区域分为若干个独立的运动单元。ActionUnite（ActionUnite，AU）通过AU的运动强度和不同AU之间的位置关系来描述表面。表情。2002年，Ekman等人开发了Micro-RepresentationTrainingTool(METT)。从业者识别和检测微表情[11]。2009年，Frank等人要求受试者表演微表情的视频片段。识别和标记，受过工具训练的人与未受过METT训练的人相比的微表情识别分类能力有了很大提高，准确率达到50%[18]。因此，已经进行了有效改善微表情的研究。识别准确度的方法尤为重要。
面部表情肌肉的微小变化成为微表情。主要分为三个阶段：Onset Frame、Apex Frame、End Frame，也就是起始帧、顶点帧、结束帧。如图1.1所示，该图是用三种帧中像素之间的绝对差来表示的。可以看出微表情的轨迹像抛物线一样移动。分析这一运动过程对于有效提高和提高微表情的识别准确率具有十分重要的意义。
图1.1 微表情的运动轨迹![](https://cdn.nlark.com/yuque/0/2022/jpeg/22897659/1670997506776-d48e0620-06ea-42df-8724-d4b173fe665d.jpeg#averageHue=%23faf9f9&crop=0&crop=0&crop=1&crop=1&id=VXsE8&originHeight=1001&originWidth=1335&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
这里可以将微表情面部肌肉开始运动的帧称为微表情起始帧，微表情面部肌肉结束的帧称为微表情结束帧。微表情面部肌肉运动最强的那一帧称为微表情顶点帧。顶点框架[19]。 开始帧之前和结束帧之后的状态称为平静的面部状态。这是因为人的心理在这个时期不受外界的刺激，或者说受到刺激后心态趋于稳定。温和的，恰恰相反，当心灵受到某种刺激时，主体会不自觉地产生微表情。然而，与此同时，受试者试图抑制或隐藏这种情绪状态的唤起 [7]。
研究学者们对于提高微表情识别的准确性提出了许多方法。比如用于提取微表情特征的方法就有光流、动态纹理、时空域。用于微表情分类早期的方法主要有 SVM、贝叶斯。但是随着深度学习技术的不断发展，微表情研究者们也提出了各种各样的新方法来提高微表情识别的准确性。2017年双时间尺度卷积神经网络来对微表情识别进行分类[26]，但是器准确性却不是特别的高。同时微表情有着不同于其他图像识别的特点就是运动强度低、持续时间短，这就造成微表情特征提取的方法依旧没有十分优秀的方法。而且深度学习也没有着重分析微表情开始帧、顶帧、结束帧，尤其对于微表情这类面部肌肉运动强度本身就很微小的运动来说，研究这帧变化确实十分重要的
#### 0.2 国内外研究现状
面部微表情不受人们内心的主观控制，并且能有效的揭示人们内心的真实情感，因此近年来面部微表情受到越来越多研究者的关注。
微表情变化速度快、持续时间短、面部肌肉运动幅度小，通常使用高速相机来拍摄微表情建立微表情数据集。2009 年，由于实验所需，Polikovsky 等人通过填写问卷等方式来诱发被试的面部微表情，建立了微表情数据库（Polikovsky’s database）[[27]](#_bookmark67)。在数据采集的过程中，要求被试按照 7 种表情类别分别做出对应的微表情后迅速回归到面部平静状态，并且这个回归过程被要求尽可能的快。对于微表情识别任务，Polikovsky 首先将微表情帧分割为 12 个子区域包括前额、左眉、右眼、嘴、左嘴角等，并对每个区域使用 3D 梯度直方图提取特征，随后通过 K 均值聚类算法完成对微表情的识别分类。虽然该方法在微表情识别上取得了较好的效果，但实验所用的数据有较大的缺陷，对于真正的微表情而言，该数据库有很明显的摆拍痕迹，并不是真正意义上的微表情。Wu 等人设计了 Gabor 滤波器来提取微表情特征结合 GentlesSVM  分类器完成微表情识别分类[[28]](#_bookmark68)。该方法
将宏表情图像作为训练集，使用 METT 中的微表情视频段作为测试集，最终微表情识别率达到了 85.4%[[11]](#_bookmark53)。但是，该实验所用的 METT 数据集由人工合成，并不是真正意义上的微表情。
早先关于微表情识别的研究中，由于缺乏有效的自发微表情数据集，实验多数使用了模仿性的微表情数据。在采集模仿性的微表情数据过程中，被试会根据实验者的指示摆拍出面部微表情。然而，这样的模仿性微表情对于真正意义上的微表情是相悖的，真正的微表情有明确的面部运动规律和清晰的面部运动过程；另一方面，以摆拍的方式建立的微表情数据集人为的抑制了其它面部运动（如：眨眼等）对识别微表情带来的影响，使得微表情识别率结果更好。模仿性的微表情与自发微表情有本质上的区别，模仿性的微表情是由人们主观控制产生的，而自发微表情是人们在试图隐藏其自身想法的时候无意识流露出的面部肌肉运动，是不可控制和不可模仿的。
为了避免模仿性数据库给微表情识别带来的弊端，近年来微表情领域相继出现了自发微表情数据集。2013 年，Li 等人建立了微表情领域的第一个自发微表情数据库 SMIC，该数据库通过音乐、电影等方式来刺激被试者心理的情绪反应，另一方面又要求被试尽可能去抑制这种情绪的流露[29]。该数据集对 16 个实验被
试共记录了 164 段微表情视频，采集的微表情类型包括正样本、负样本和惊讶三种。同年，Yan 等人以同样的诱发方式发布了自发微表情数据集 CASME I[[30]](#_bookmark70)。该数据集由两个部分组成，第一部分的分辨率为 1280×720；第二部分的分辨率为 640×480，共记录了 195 段微表情视频，数据集中包括厌恶、惊讶、蔑视等 8 种情绪类型的微表情，同时也标明了每个微表情序列的面部运动单元、开始帧、顶帧和结束帧。2014 年，Yan 等人在 CASME I 的基础上，发布了具有普遍性意义的 CASMEⅡ[31]。该数据集共记录 26 个被试的 255 个微表情样本，和 CASME I一样，CASME Ⅱ数据集也标注了微表情的标签、面部运动单元以及微表情序列的开始帧、顶帧和结束帧。2016 年，Davison 等人发布了自发微表情数据库SAMM[[32]](#_bookmark72)。在数据采集之前，他们为每一位被试单独设计了情绪诱发材料以保证受试者有良好的情绪反应，该数据集对 32 名被试共记录了 159 段微表情数据样本，平均年龄
约为 33.24 岁，包括来自 13 个不同种族的人。数据集包括生气、惊讶、恐惧等 7种情绪类型的微表情，同时也注明了每个微表情样本的面部运动单元、开始帧、顶帧以及结束帧。
随着越来越多的研究者致力于微表情识别的研究，微表情识别方法大体可以分为以下几种：
（1）基于时空域纹理提取微表情特征的方法。2011 年，Pfister 等人在应用时空局部纹理描述特征的基础上利用 LBP-TOP 进行微表情识别分类。该方法使

第一章 引言
用时间插值模型（TIM）将数量不同的微表情序列归一化后，同时结合多核学习机（MKL）完成微表情正性和负性样本二分类并且识别率达到 71.4%[[33]](#_bookmark73)。Yan 等人利用LBP-TOP 提取微表情特征结合SVM 分类器完成微表情识别，在 CASME Ⅱ数据集上五分类的识别结果达到了 63.41%[[31]](#_bookmark71)。Wang 等人对于微表情特征提取提出了 6 点交叉局部二值模式（LBP-SIP）特征并结合 SVM 分类器完成微表情识别，其识别率达到了 67.2%，LBP-SIP 和 LBP-TOP 比较而言其最大的优势在于减少了大量冗余信息，提升了微表情的识别效率[[34]](#_bookmark74)。2015 年，Huang 等人利用积分投影的方法将微表情帧分解为水平投影和垂直投影，在此基础上利用局部二值模式
（LBP）提取水平投影和垂直投影的特征，并且在三个数据库上验证了该方法的有效性[35]。Kamarol 等人利用时空特征图（STTM）来表示微表情特征并结合 SVM分类器完成微表情分类识别，在 CASME Ⅱ上 4 分类识别精度达到了 91.71%[[36]](#_bookmark76)。 Park 等人提出了一种基于时间特征的自适应放大细微面部动作并结合 LBP-TOP在 CASME Ⅱ上取得了较好的识别结果[37]。2016 年，Duan 等人分析了眼部区域对于微表情识别的有效性并且结合 LBP-TOP 提取眼部区域的特征，该方法在 CASME Ⅱ上取得了较好的识别结果[38]。
（2）基于时空域运动信息描述的方法。2014 年，Liong 等人利用光流应变来提取微表情序列帧之间的运动特征，并对求得的光流应变图特征进行求和池化，使得该特征更具有代表性并在 SMIC 数据集上验证了该算法的有效性[39]。随后 Liong 等人在光流的基础上提出了光流应变加权（OSW）算法来提取微表情特征，首先根据光学应变大小对光学应变图像进行计算和归一化，将空间平面（XY）划分为 N×N 个不重叠的块，得到单个块的幅值，随后将特征提取器 LBP-TOP 与光学应变权值相乘，形成最终的特征直方图，并用 SVM 完成微表情特征分类，结果表明该算法在 CASME Ⅱ五分类识别率达到了 66.4%，在 SMIC 上三分类识别率达到了 57.7%[[40]](#_bookmark80)。Wang 等人利用鲁棒性 PCA（RPCA）提取微表情的稀疏特征信息，为提取微表情局部动态纹理特征通过 FACS 将面部分为 16 个感兴趣区域并利用 LSTD 来完成，实验结果表明在 CASME Ⅱ的识别率达到了 65.45%[[41]](#_bookmark81)。2015 年， Zhang 等人在微表情相邻帧之间采用光流技术提取特征与LBP-TOP 提取的微表情时空局部纹理特征相结合，实验结果表明在 CASME Ⅱ上识别率达到了 64.46%[[42]](#_bookmark82)。 2016 年，Ngo 等人运用欧拉放大技术并结合 LBP-TOP 提取微表情特征，该方法在微表情识别中取得了良好的效果[43]。Liu 等人将主方向平均光流（MDMO）特征用于微表情识别，同时将面部感兴趣区域划分为 36 块后运用光流法统计微表情运动信息及其空间位置，最终通过光流特征向量来完成微表情分类，实验结果表明在 CASME、CASME Ⅱ和 SMIC 上的识别率分别为 68.8%、67.37%和 80%[[20]](#_bookmark62)。 2017 年，Xu 等人提出了一种面部动态映射（FDM）方法来表征不同粒度的微表

情运动，为将微表情序列达到像素级对齐采用了光流技术，最后在公开数据库上验证了该算法对微表情识别的有效性[[8]](#_bookmark50)。2018 年，Liong 等人对于微表情特征提取提出了双向加权定向光流（Bi-WOOF）的方法，而该方法仅使用一段微表情视频序列中的顶帧完成微表情识别任务且取得了实验预期的效果[44]。

1. 基于高阶张量提取微表情特征的方法。2014 年，Wang 等人对于微表情特征提取设计了一种基于判别式张量子空间分析（DTSA）算法，该算法将灰度人脸图像作为二阶张量，通过保存图像的空间结构信息，并结合极限学习（ELM）完成对微表情识别分类。实验结果表明该方法在 CASME Ⅰ上的识别率为 46.9%[[6]](#_bookmark48)；同年，Wang 等人对于微表情特征提取又设计了一种将 RGB 转换为张量独立颜色空间（TICS）的方法[45]。将微表情视频序列分解为一个四维张量，前两个维度主要包括时间信息，第三个维度表示空间信息，第四个维度表示颜色信息，同时根据 FACS 将微表情面部区域划分为 16 个感兴趣区域（ROI），并对每一个 ROI 计算动态纹理直方图特征，最后通过 SVM 分类器在 CASME  Ⅰ和 CASME  Ⅱ验证了该算法的识别精度分别达到了 61.8%和 61.7%。2015 年，Wang 等人在张量独立颜色空间（TICS）的基础上揭示了另外两种颜色空间（CIELab 和 CIELuv）也有助于改善微表情识别率[[46]](#_bookmark86)。2016 年，Wang 等人对于微表情特征提取提出了稀疏张量正则相关性分析（STCCA）方法，利用 STCCA 来寻找一个子空间使得微表情数据与其对应的 LBP 特征的相关性在该子空间中最大，进一步对该子空间计算泛化后得到具有稀疏性的微表情特征[47]。Ben 等人提出用张量来表示最大边距投影

（MMPTR），通过求得 MMPTR 可以寻找一个张量到张量的投影，可直接从原始微表情张量数据中提取更具有区分性的特征[[48]](#_bookmark88)。

1. 基于深度学习的方法。随着深度学习在图像识别与目标检测中的成功运用，越来越多的研究者运用深度学习来处理微表情。2017 年，Peng 等人提出了双时间尺度卷积神经网络（DTSCNN）方法，该方法通过两个输入通道来处理不同的微表情帧率，同时将光流作为神经网络的输入进一步确保该网络能学习到更高水平的特征，最后通过 SVM 分类器在 CASME  Ⅰ和 CASME  Ⅱ上验证该算法的识别率达到了 66.67%[[26]](#_bookmark66)。2018 年，Peng 等人利用迁移学习的方法在宏表情数据集上微调模型参数，同时在微表情数据集 CASME Ⅱ和 SAMM 上训练和验证并取得了很好的效果，且该方法在微表情识别挑战赛上赢得了微表情识别大赛冠军[49]。 Wang  等人提出了一种与残差网络协同学习的微注意力机制，该机制能够有效的使神经网络聚焦于面部感兴趣的运动单元并学习聚焦区域的特征，同样运用迁移学习的方法在 CASME  Ⅱ、SAMM 和 SMIC 上取得了较好的识别效果[50]。Liong等人对于微表情特征提取提出了光流顶帧网络（OFF-ApexNet），该方法将光流引导的上下文特征与 CNN 相结合，并在 SMIC、CASME Ⅱ和 SAMM 上验证了该算

第一章 引言
法的有效性，识别率达到了 74.6%[[51]](#_bookmark91)。Wang 等人提出迁移学习长短期卷积神经网络（TLCNN），该方法利用 CNN 提取微表情序列的每一帧特征信息，接着将该特征输入到长短期记忆（LSTM）网络以进一步学习微表情的时间序列特征信息[52]。 2019 年，Liong 等人在光流特征的基础上提出了对抗生成网络（GAN）的方法来克服微表情样本量少的缺陷[[53]](#_bookmark93)。Song 等人提出了三通道卷积神经网络（TSCNN）方法，三个通道分别是静态空间流、动态时间流以及局部空间流，同时融合微表情的时间、空间以及微表情局部特征来提高微表情的识别率[[54]](#_bookmark94)。2020  年，Peng等人提出了一种自适应关键帧挖掘网络（AKMNet），该方法主要利用局部关键帧信息及全局时间动态特性来提取微表情特征改善微表情识别精度[[55]](#_bookmark95)。Lei  等人提出图时间卷积网络（Graph-TCN）来提取面部微表情的局部区域特征，通过人脸关键点定义图结构，该图结构的一个通道用于节点特征提取，另一个通道用于边缘特征提取，随后将边缘特征和节点特征融合分类[[56]](#_bookmark96)。Chen 等人提出基于卷积块注意模块的 3D 时空卷积神经网络方法（CBAMnet），该方法首先用 CNN 提取微表情序列的视觉特征，随后借助卷积块注意模块自适应学习微表情特征权重[[57]](#_bookmark97)。
#### 0.3 论文研究内容及创新点

      1. 论文的研究内容

本文提出了基于多通道卷积神经网络和胶囊网络模型的微表情自动识别方法，分析研究了微表情开始帧、顶帧以及结束帧对微表情识别的影响，研究内容主要包括以下几个方面：微表情数据库及其预处理、基于胶囊网络模型的多通道卷积神经网络的建立、自发微表情的分类与识别结果分析。

         1. 自发微表情数据库及其预处理。本次实验主要使用中国科学院心理研究所采集发布的自发微表情数据库 CASME  Ⅱ和曼彻斯特城市大学情感智能学院发布的微表情数据库 SAMM；首先需要对数据集进行预处理，预处理的步骤包括：首先，根据面部区域的 68 个关键特征点来检测图像中的人脸，同时将检测到的人脸图像从原始图像中裁剪出来。其次，在裁剪得到的人脸图像中，基于模型标定的左眼角、鼻尖、嘴角等 5 个关键点完成人脸矫正。最终在每一张人脸图像矫正之后，对开始帧、顶帧以及结束帧之间分别提取光流运动信息，得到相应的光流图像。
         2. 基于胶囊网络模型的多通道卷积神经网络的建立。在完成微表情预处理之后得到微表情开始帧、顶点帧和结束帧之间的光流图像，本实验采用五通道卷积神经网络对得到的光流图像进行特征提取。输入的光流图像包括开始帧和顶帧之间的水平及垂直光流图像，顶帧和结束帧之间的水平及垂直光流图像。同时，

为了更进一步加强微表情特征信息，微表情顶帧的灰度图像也作为网络的一个输入。在完成微表情特征提取后将五个通道的特征进行融合，随后将融合的特征作为胶囊网络的输入，通过胶囊网络的动态路由机制来完成微表情识别分类。

         1. 自发微表情的分类与识别结果分析。在微表情识别上，将卷积神经网络作为微表情的特征提取描述器，同时将五通道卷积神经网络的特征融合后输入到胶囊网络中完成微表情分类识别；为了验证本文设计的方法对微表情识别效果的有效性，我们将上面的方法与下面四种进行了对比：开始帧和顶帧的光流加上微表情顶帧的灰度图像作为一个三通道网络的输入并用胶囊网络完成微表情分类；顶帧和结束帧的光流加上微表情顶帧的灰度图像作为一个三通道网络的输入并用胶囊网络完成微表情分类；仅微表情顶帧的灰度图像作为网络的输入并用胶囊网络完成微表情分类；仅微表情顶帧的灰度图像作为网络的输入并用 Softmax 完成微表情分类。实验结果表明了我们提出的方法对微表情分类识别的准确率起到了明显的改善作用。
      1. 论文的创新点

根据文献调查可以发现，大多数方法使用微表情的顶帧或微表情开始帧和顶帧来完成微表情识别分类。但是，在进行微表情分类识别时大多数研究者忽略了微表情结束帧给微表情识别分类带来的影响，此外对微表情的运动变化细节信息也缺乏关注。针对以上问题，本文在自发微表情识别上的创新之处如下：

1. 提出五通道卷积神经网络，探索微表情运动变化对微表情识别的影响，为避免网络在小样本量上训练出现过拟合，网络的每个通道由三个卷积层和三个池化层组成，随后将五个通道的特征融合后进一步加强微表情特征。
2. 结合胶囊网络模型进行特征增强，提高识别效果，使得网络不深的情况下仍然能够学习到丰富的特征，同时完成微表情最终分类识别。在样本量较少的情况下，通过留一被试法验证了该方法在微表情识别分类上仍然有很好的效果。
#### 0.4 论文的结构安排
全文一共分为五个部分，详细介绍如下：
第一章为引言。主要介绍了微表情相关的研究背景以及利用计算机领域中模式识别等相关算法对改进微表情识别分类准确率的实际影响意义，其次对目前国内外发布的自发微表情数据库和国内外微表情识别的研究进行了简单梳理，并指出了现有研究中微表情识别存在的不足之处，同时针对现有研究方法中存在的缺陷且有待改进的地方，提出了本文的主要研究内容和方法上的创新。
第二章为自发微表情数据库及其预处理。首先对实验所需的两个数据库

第一章 引言
（CASME Ⅱ、SAMM）进行了简单介绍。接着对微表情图像使用 Dlib 算法完成了人脸检测、裁剪以及矫正，并对矫正后的图像进行了尺寸归一化处理。根据面部微表情的运动特点，在尺寸归一化后的微表情的开始帧、顶帧及结束帧之间提取光流图像。
第三章为基于五通道卷积神经网络和胶囊网络的建立。这章首先介绍了深度学习、卷积神经网络以及胶囊网络的基本概念以及原理，随后在该理论的基础上提出了本文对于微表情识别的方法，同时分别讲述了五通道卷积神经网络和胶囊网络模型的建立及其原理和优势。
第四章为实验测试结果与分析。将搭建好的神经网络在预处理后的数据集上完成调参训练。随后将训练好的神经网络模型作为特征提取描述符来提取微表情特征。引入胶囊网络完成微表情最终识别分类同时采用留一被试法作为验证方式，并与其它方法进行了对比，进一步验证了本文所提算法的有效性，将基于胶囊网络的五通道卷积神经网络、基于胶囊网络的三通道卷积神经网络与基于胶囊网络的单通道卷积神经网络进行了识别结果上的对比。
第五章为总结与展望。这部分首先对本文的研究工作进行了回顾总结。分析其优点和不足，最后对微表情识别分析的发展进行展望。


![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997507392-8a26c8f6-11fe-499e-8e0d-6101da28b7e3.png#averageHue=%23ffffff&crop=0&crop=0&crop=1&crop=1&id=xSZQ3&originHeight=3508&originWidth=2481&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997507845-15187e62-903b-4224-a1c8-8ac09e2e5f22.png#averageHue=%23ececec&crop=0&crop=0&crop=1&crop=1&id=QMq2E&originHeight=26&originWidth=441&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997508113-cd328349-4765-4fde-a112-c2bfea612e90.png#averageHue=%23ececec&crop=0&crop=0&crop=1&crop=1&id=DY7ex&originHeight=26&originWidth=555&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997508433-8caa7f1a-b187-4e1f-b540-1969d60d8115.png#averageHue=%23ededed&crop=0&crop=0&crop=1&crop=1&id=XktwW&originHeight=26&originWidth=227&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)

第二章 自发微表情数据库及其预处理

### 第二章 自发微表情数据库及其预处理
当人们内心受到某种外来刺激时面部肌肉无意识的运动会产生微表情，此时微表情能有效反应人们内心的真实想法。在微表情识别的研究中，有效可靠的自发微表情数据能直接影响到微表情识别的最终结果，同时，自发微表情数据库也是验证微表情识别算法有效性和微表情识别结果可靠性的重要前提。本章首先对实验所需的自发微表情数据集进行介绍，随后对微表情的预处理方法进行简单的叙述。
#### 0.5 自发微表情数据库
本次实验所用数据集包括中国科学院心理研究所采集发布的自发微表情数据库 CASME Ⅱ[[31]](#_bookmark71)和曼彻斯特城市大学采集发布的自发微表情数据库 SAMM[[32]](#_bookmark72)，两个数据库的详细介绍如下。
2014 年，中国科学院心理研究所 Yan 等人建立并发布了自发微表情数据集 CASME Ⅱ[[31]](#_bookmark71)。在采集数据时，使用了 4 盏大功率且光源稳定的 LED 灯作为照明光源，同时为了减小环境带来的干扰，将 4 盏大功率的 LED 置于黑色伞下，数据库由 Point Grey GRAS 03K2C 相机拍摄，帧速率设置为 200fps，分辨率设置为 640
×480。CASMEⅡ由 26 个被试的 255 个自发微表情视频片段组成，将视频作为材料让被试观看来刺激被试的心理从而诱发微表情，通过不同的视频类型能有效诱发被试的不同情绪。此外，CASMEⅡ的微表情样本标记了微表情的开始帧、顶帧、结束帧、AU 及情感标签。需要指出的是，微表情样本的情绪标签由 AU、视频情感类型以及被试反馈的心理情绪状态共同决定。自发微表情数据库 CASME Ⅱ的部分基本信息如表 2.1 所示。
表 2.1 CASME Ⅱ数据库部分基本信息
数据库	个体	平均年龄	样本数	帧速率	分辨率	情感类别（数量）

厌恶(63) 压抑(27)

CASME Ⅱ	26	22.03	255	200fps	640×480

高兴(32) 惊讶(25)

害怕(2) 悲伤(7)


其他(99)
2018 年，曼彻斯特城市大学 Davison 等人建立发布了自发微表情数据集 SAMM。在采集数据时，使用了两盏 LED 阵列的灯，为了避免日光灯 50Hz 闪烁带来的影响与噪声，在 LED 光源周围放置了光线漫射器使得被试脸上的光线照射更均匀，数据库由 Basler Ace acA2000-340km 相机拍摄记录同时添加了帧捕捉器和固态驱动器的 RAID 阵列来捕获图像，以确保没有帧丢失发生，该数据库的样
11

本分辨率达到了 2040×1088，相机的帧速率设置为 200fps。与 CASME  Ⅱ自发微表情数据库比较而言，SAMM 由不同种族、不同年龄段的 32 个被试共 157 个自发微表情视频片段组成，与 CASMEⅡ的刺激方式不同的是，SAMM 数据库在采集时虽然也是将视频作为材料让被试观看来刺激被试的心理从而诱发微表情，通过不同的视频类型能有效诱发被试的不同情绪，但作者为每个被试都量身定制单独的视频刺激材料使得被试的情绪能够被充分的诱发。此外，SAMM 的微表情样本标记了微表情的开始帧、顶帧、结束帧、AU 及情感标签，图 2.1 展示了 SAMM数据库中“惊讶”微表情示例。自发微表情数据库、SAMM 的部分基本信息如表

   1. ![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997509062-e9fc3f28-2ab6-4033-9ee8-06ab9cece259.png#averageHue=%23858585&crop=0&crop=0&crop=1&crop=1&id=Q7K0P&originHeight=159&originWidth=1173&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)所示。


(a)
(b)
(c)
(d)
(e)

图 2.1 SAMM 数据库中“惊讶”微表情: (a) 微表情开始帧; (c) 微表情顶帧; (e) 微表情结束帧
表 2.2 SAMM 数据库部分基本信息
数据库	个体	平均年龄	样本数	帧速率	分辨率	情感类别（数量）

厌恶(9) 生气(57)
SAMM	32	33.24	157	200
2040×
1088
高兴(26) 惊讶(15)
恐惧(6) 悲伤(6)


蔑视(12) 其他(26)
本次实验所用的自发微表情数据集包括 SAMM 和 CASME Ⅱ，在两个数据集上分别进行五个类别的分类实验并选择样本量多的微表情样本进行本次实验，实验数据详细情况如下表 2.3 所示。
表 2.3 实验数据详细情况
数据库	样本数	情感类别（数量）



CASME Ⅱ	246
厌恶(63) 高兴(32) 压抑(27)惊讶(25) 其他(99)

SAMM	136
生气(57) 高兴(26) 其他(26)惊讶(15) 蔑视(12)

#### 0.6 数据库预处理
微表情是人们无意识地在心理受到刺激的影响下出现的面部肌肉运动，具有持续时间短、运动幅度低等特点。在自发微表情数据的采集过程中，存在一些与
12

第二章 自发微表情数据库及其预处理
微表情识别无关的因素如咽口水、眨眼等干扰，这些干扰因素会对微表情识别结果带来很大的影响。由此可见，在使用公开的自发微表情数据库之前，对每一个微表情样本进行预处理是必不可少的，同时运用深度学习完成微表情识别分类时需要对微表情图像进行尺寸归一化。本实验中使用的数据库主要是 CASME Ⅱ和 SAMM，微表情预处理主要包括：人脸检测、人脸剪裁、人脸矫正以及尺寸归一化等，预处理主要流程如下图 2.2。







计算图像光流
人脸图像尺寸归一化
人脸矫正

![](https://cdn.nlark.com/yuque/0/2022/jpeg/22897659/1670997509441-ee44d59e-7a6c-48bb-a969-129b0bc5598d.jpeg#averageHue=%237089a8&crop=0&crop=0&crop=1&crop=1&id=FGz8E&originHeight=206&originWidth=273&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)图 2.2 微表情预处理流程
人脸检测
人脸裁剪

      1. 人脸关键点检测及裁剪

影响自发微表情识别分类结果的因素多种多样，如环境、背景噪音、头部晃动及咽口水等。消除这些不利因素对改善微表情识别分析工作十分有益。根据 FACS（Facial Action Coding System），微表情产生主要集中在面部区域的某一些局部区域，表 2.4 所示给出了部分自发微表情对应的面部运动单元（AU），不同的面部肌肉运动单元可以组成不同类型的微表情。在微表情图像中检测到人脸时并把人脸区域裁剪出来，这时就意味着有效地消除了大部分环境、背景噪音等不利因素带来的影响，由此可见人脸检测与裁剪在微表情识别中发挥着至关重要的作用[[58, ](#_bookmark98)[59]](#_bookmark99)。在检测人脸的工作中有两种方法，主要包括手动检测和自动检测。然而微表情大样本量给手动检测带来了一定的困难且耗时费力，故本实验对于人脸检测采取自动检测的方式，主要利用 Dlib 工具包来完成这项工作[60]。Dlib 中的人脸检测算法和其他人脸检测算法相比具有速度快且鲁棒性好等特点，可以精准、快速地捕捉到图像中的人脸区域。Dlib 检测人脸的 68 个关键点如图 2.3（a），根据 68 个关键点的位置，在图像上的人脸矩形框如图 2.3（b）所示。在人脸裁剪时，借助人脸的矩形框来确定人脸范围完成人脸裁剪，裁剪后的人脸如图 2.3（c）。







13
![](https://cdn.nlark.com/yuque/0/2022/jpeg/22897659/1670997509951-65e4ecbe-35e7-4044-a0a4-af27f7f4055c.jpeg#averageHue=%237089a8&crop=0&crop=0&crop=1&crop=1&id=ZbRS1&originHeight=252&originWidth=337&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/jpeg/22897659/1670997510330-6303c0e5-3788-45b5-8dfb-e7b9fb718715.jpeg#averageHue=%237b97ba&crop=0&crop=0&crop=1&crop=1&id=q1l3C&originHeight=250&originWidth=337&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
![](https://cdn.nlark.com/yuque/0/2022/jpeg/22897659/1670997510657-55b6dc23-8531-458d-aa61-2dc24a1ebeed.jpeg#averageHue=%236e87a2&crop=0&crop=0&crop=1&crop=1&id=op8lw&originHeight=321&originWidth=321&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)

         1. (b)	(c)

图 2.3 人脸关键点检测: (a) 人脸的 68 个关键点; (b) 绘制矩形框; (c) 人脸裁剪表 2.4 不同的面部肌肉运动对应的情绪类型
AUs	情绪
AU6 或 AU12	高兴
AU9 或 AU10 或 AU4+AU7	厌恶
AU1+AU2，AU25 或 AU2	惊讶
AU15 和 AU17 单独或组合	压抑
其他面部运动产生的微表情	其他

      1. 人脸图像配准及尺寸归一化

在拍摄微表情的过程中，实验时间过长或者被试身体疲惫会引起身体晃动、头部转动等影响微表情识别结果的不利因素，而微表情主要集中在面部区域，即使头部转动偏移的角度很小，也会对微表情识别分类的结果有很大影响[[61, ](#_bookmark101)[62]](#_bookmark102)。因此，人脸配准对改善微表情识别分类的结果发挥着十分重要的作用，而对于图像配准一般使用数学意义上的仿射变换来完成，仿射变换通常包括缩放、平移、旋转、反射等。总的来说，仿射变换具有凸性、共线性、平行性、共线比例不变性，仿射变换公式如（2.1），即原来是一条平行线经仿射变换之后得到的仍然是一条平行线；原来是一条直线经仿射变换之后得到的仍然是一条直线。在对人脸配准进行仿射变换时，需要先得到人脸特征点坐标、角度等信息，接着再对图像计算仿射变换矩阵，最后完成对图像、区域、轮廓的仿射变换。

		        		  (2.1)
其中， _x _和 _y _是变换前的特征点坐标，经过仿射变换之后的特征点坐标是_u _和_v _。从仿射变换公式得到，图像平移时对应的公式如（2.2）。


					(2.2)



14

第二章 自发微表情数据库及其预处理

同理，旋转和图像伸缩对应的公式分别为（2.3）和（2.4）。




(2.3)

(2.4)




当微表情数据样本中存在头部偏移时，经仿射变换后，可以有效消除头部转动给识别分类带来的影响，通常情况下，式（2.4）可以扭转大部分头部偏移带来的影响。在 Dlib 工具箱中已包含了带有 5 个关键点的标准人脸作为模板，5 个人脸关键点包括两个外眼角、两个内眼角以及鼻尖作为待配准的关键点，如图 2.4
（b）所示。在 Dlib 人脸配准的算法中，首先通过人脸检测器获得微表情图像中的人脸关键点，随后与标准库中人脸模板进行关键特征点一一比对，同时通过仿射变换得到待配准人脸关键点的变换矩阵，最后将变换矩阵应用到待配准的图像中得到最终的结果，矫正后的人脸图像如图 2.4（c）所示。人脸矫正过程会根据检测到的关键点与标准模板人脸对比后进行图像微小的旋转进行矫正，当微小的偏移旋转完成之后，会存在部分黑色区域在图像中属于无关元素，因此我们可根据人脸检测算法再次将图像进一步剪裁去除不相关的信息。本实验中，将配准后的微表情图像尺寸归一化到 224×224 以方便卷积神经网络处理，如图 2.4（d）。
![](https://cdn.nlark.com/yuque/0/2022/jpeg/22897659/1670997510986-16c3a41d-40fd-4474-a3df-6766f761fa06.jpeg#averageHue=%236e87a2&crop=0&crop=0&crop=1&crop=1&id=ch6xb&originHeight=321&originWidth=321&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/jpeg/22897659/1670997511221-c3bfa8f7-e42c-46e2-b561-e43917e3be47.jpeg#averageHue=%23171d1c&crop=0&crop=0&crop=1&crop=1&id=fX2TG&originHeight=320&originWidth=320&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/jpeg/22897659/1670997511521-023c150f-c474-4e77-a75a-194080f93bc5.jpeg#averageHue=%236e88a3&crop=0&crop=0&crop=1&crop=1&id=bzEZt&originHeight=321&originWidth=321&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)224×224
![](https://cdn.nlark.com/yuque/0/2022/jpeg/22897659/1670997512088-22240960-8c48-4a59-a82c-919543edf4a6.jpeg#averageHue=%23607285&crop=0&crop=0&crop=1&crop=1&id=w4Un0&originHeight=224&originWidth=224&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
(a)	(b)	(c)	(d)

图 2.4 人脸配准及尺寸归一化: (a) 待矫正人脸图像; (b) 待配准的 5 个关键点; (c) 配准后的图像; (d) 归一化图像

      1. 计算光流

在计算机影像领域，特别是在运动图像中，图像的亮度变化会引起像素点表面的变化，这种像素点的瞬时速度场称之为光流，而光流主要用来处理分析视频中连续两帧之间的运动信息。通常情况下，分析图像的像素运动主要通过光流模型求得图像像素点的瞬时速度场称之为图像的光流场或者图像的速度场。对于运动的图像求得光流图像信息后，再提取特征是十分有效的[63]。1981 年 Horm 等人提出了光流的计算方法，通过图像与运动场之间的关系建立光流约束方程引出了
15
基本的光流算法[64]。因此，图像的运动信息可以用光流来近似表示，再结合卷积神经网络在目标检测[[65, ](#_bookmark105)[66]](#_bookmark106)、目标分类[67, [68]](#_bookmark108)等任务表现，将会取得良好的效果。
在一般情况下，光流能较好地表示运动图像的特征信息。本实验主要通过梯度的方法来计算光流，对于实验所用到的微表情图像，我们假设 _I _ _x_, _y_, _t _ 表示该图像在_t _时刻的像素运动情况，当该像素点运动到 _x _ __ _x_, _y _ __ _y _ 时，此时运动时
间为 __ _t_
， 因 此 在 该 图 像 的 后 一 帧 微 表 情 像 素 点 的 运 动 情 况 为

_I _(_x _ __ _x_, _y _ __ _y_, _t _ __ _t_) 。根据光流法的亮度恒定条件，在极短时间内，在时间__ _t _的前后图像中某像素点的亮度值保持不变，可得式（2.5）：

_I _ _x_, _y_, _t _  _I _ _x _ __ _x_, _y _ __ _y_, _t _ __ _t _
(2.5)


当在时间__ _t _前后微表情像素点的两个运动方向（水平运动和垂直运动）的速率分别用_u_(_x_, _y_) 和_v_(_x_, _y_) 来表示，则有__ _x _ _u_ _t_,__ _y _ _v_ _t _。同时，假设微表情图像的
像素点的亮度关于时间和位置的关系按泰勒展开为（2.6）：

_I _ _x _ __ _x_, _y _ __ _y_,_t _ __ _t _  _I _ _x_, _y_,_t _  __ _x __I _ __ _y __I _ __ _t __I _ __
(2.6)
_x	__y	__t_
在式（2.6）中， __ 表示对__ _t _的二阶无偏估计，当__ _t _足够小时，联立式（2.5）和
（2.6）可得式（2.7）：

__ x _I _ __ _y __I _ __ _t __I _ 0
(2.7)

_x	__y	__t_
将__ _x _ _u_ _t_,__ _y _ _v_ _t _代入式（2.7）可得：
_u __I _ _v __I _ _I _ 0


(2.8)

_x	__y	__t_
式（2.8）即为光流运动方程，式中_u_、_v _表示光流矢量在两个方向（水平和垂直）上的光流运动分量。
尽管微表情变化速度快、持续时间短且运动区域仅集中在面部的小部分区域，但光流算法仍然能够捕捉到微表情的细微特征的运动信息。也就是说，利用光流估计面部区域的微运动在很大程度上决定了微表情识别的准确性。本实验使用 Liu 等人提出的光流算法[69]。在图 2.5 中，选取了 CASME Ⅱ微表情数据以 sub01-EP02_01f 为例，我们提取了该样本的水平光流图像和垂直光流图像。这个样本的微表情标签被标记为高兴。如图 2.5 所示，第一行表示起始帧、顶点帧、
偏移帧以及各自在微表情数据库中的索引。图 2.5 中的第二行是顶帧与各帧之间的水平和垂直光流场。







16

第二章 自发微表情数据库及其预处理
Onset-Frame-46	Apex-Frame-59	Offset-Frame-86
![](https://cdn.nlark.com/yuque/0/2022/jpeg/22897659/1670997512630-493d911a-83e0-4975-b050-cfd1c04e1f25.jpeg#averageHue=%23607285&crop=0&crop=0&crop=1&crop=1&id=xsyMA&originHeight=224&originWidth=224&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/jpeg/22897659/1670997513183-d541bf1c-54b8-4f24-a361-78760dee5b3d.jpeg#averageHue=%23697d93&crop=0&crop=0&crop=1&crop=1&id=SXVL2&originHeight=224&originWidth=224&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/jpeg/22897659/1670997513678-8e13899b-3ab5-403c-a662-671a84d39e0e.jpeg#averageHue=%236a7f95&crop=0&crop=0&crop=1&crop=1&id=zz73O&originHeight=224&originWidth=224&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/jpeg/22897659/1670997514180-5ced76a8-18bd-4ca7-aecf-6db5aed498bf.jpeg#averageHue=%23000000&crop=0&crop=0&crop=1&crop=1&id=RKRrI&originHeight=224&originWidth=224&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/jpeg/22897659/1670997514623-6cac0265-49aa-4af7-96b1-66c3b8afb5f3.jpeg#averageHue=%233b3b3b&crop=0&crop=0&crop=1&crop=1&id=W0Ozj&originHeight=224&originWidth=224&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/jpeg/22897659/1670997515162-3efff485-7393-4bc8-8690-bd3167e6922e.jpeg#averageHue=%235d5d5d&crop=0&crop=0&crop=1&crop=1&id=e0cdO&originHeight=224&originWidth=224&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/jpeg/22897659/1670997515431-97c12693-66c2-40a3-9203-63ed3543335c.jpeg#averageHue=%23040404&crop=0&crop=0&crop=1&crop=1&id=lsCJo&originHeight=224&originWidth=224&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)



Onset-Apex-horizontal	Onset-Apex-vertical
Apex-Offset-horizontal
Apex-Offset-vertical

(a)	(b)	(c)	(d)

图2.5 光流图像: (a) 开始帧和顶帧的水平光流; (b) 开始帧和顶帧的垂直光流; (c) 顶帧和结束帧的水平光流; (d) 顶帧和结束帧的垂直光流





























17

第三章 五通道卷积神经网络与胶囊网络的建立
### 第三章 五通道卷积神经网络与胶囊网络的建立
在前面的章节中，对实验所用两个数据库的基本情况进行了介绍，同时介绍了实验中对数据集进行预处理的流程和方法。在这一章中，将主要介绍五通道卷积神经网络以及胶囊网络模型的建立，其主要内容包括深度学习、卷积神经网络以及胶囊网络的基本原理及其基本结构。
#### 0.7 深度学习
深度学习是人工智能领域最受欢迎的方法之一。深度学习以原始数据为要素，为了得到这些数据的高水平特征通常都是通过逐层搭建神经网络来提取特征，最终将提取到的高级特征来代替原始的数据进行后续的处理[[70]](#_bookmark110)。将深度学习应用在计算机中使其来理解、模仿、辅助人类的一些行为和动作引起了研究人员的兴趣，一方面，它是基于人工神经网络的一种方法，是发展人工智能的基础，另一方面，有研究者将其应用在图像识别[[67, ](#_bookmark107)[68, ](#_bookmark108)[71]](#_bookmark111)、目标检测[65, [66]](#_bookmark106)、语音分析[72, [73]](#_bookmark113)以及机器翻译[24, [74]](#_bookmark114)等场景，并在这些方面展现出了深度学习优异的特性。接下来便以神经网络为背景，简要介绍深度学习的变化与发展进程。

      1. 神经元与感知器

大约在 1958-1969 年间神经网络经历了第一个发展阶段，其中感知器算法是最具有代表性的成果之一。在 1943 年，McCulloch 与 Pitts 设计提出了MP 神经元模型[[75]](#_bookmark115)。该模型是基于人脑神经元的数学模型，并在结构和工作原理上高度模仿人脑神经元对信息的处理过程。在 1949 年，Hebb 等人提出了一种无监督学习的规则，即海布学习规则[[76]](#_bookmark116)。海布规则通过对原始数据进行训练并提取统计数据中的特征，随后根据样本数据的特征相似度进行分类。在 MP 神经元模型和海布学习的基础上，Rosenblatt 提出了一种称之为感知器学习的算法，并于 1958 年正式提出了由两层神经元组成的神经网络即感知器[77]。感知器的基本结构如图 3.1 所示，从本质上来说，感知器是一种线性模型，通过使用梯度下降算法在对数据训练的过程中自动完成权值更新，并且能够实现对输入的训练数据集进行二分类。在图 3.1 中，每个输入信号都包括一个对应的权重 _wn _， 输入信号包括
 _x_1, _x_2 , _x_3..._xn _ ，求和节点处_vn _可表示为式（3.1）：
_n_


那么输出 _y _可以表示为：
v_n _  _wi xi _ _bn_
_i_1



_y _ _f _(_vn _)
(3.1)


(3.2)



19

偏置
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997515744-0a035175-3324-45c8-81cb-2ec4e89efd16.png#averageHue=%23a9a9a9&crop=0&crop=0&crop=1&crop=1&id=sdlOK&originHeight=14&originWidth=15&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)1
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997516221-602baf27-34d8-49d5-81dd-4dec0ca793aa.png#averageHue=%23f6f6f6&crop=0&crop=0&crop=1&crop=1&id=PTMaW&originHeight=246&originWidth=586&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
w1
bn
w2
激活函数
w3
vn
输出
求和节点
wn

![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997516777-3842a641-eeab-4154-9edd-a53d07052fdd.png#averageHue=%23a9a9a9&crop=0&crop=0&crop=1&crop=1&id=qMVBi&originHeight=15&originWidth=15&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)2
输入
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997517299-8cef668e-18cc-4499-9c47-e05c5b29b1c3.png#averageHue=%23a9a9a9&crop=0&crop=0&crop=1&crop=1&id=QpAca&originHeight=14&originWidth=15&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)信号	3


![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997517752-05a4d0ce-3f5f-47ff-9f06-73d0ed2fb559.png#averageHue=%23a9a9a9&crop=0&crop=0&crop=1&crop=1&id=a1cm2&originHeight=15&originWidth=15&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)n
突触权值

图 3.1 感知器的基本结构
式（3.2）中 _f _为激活函数，公式表达如下：
_f _ _x_   1
_if x _ 0

(3.3)

1

_otherwise_


当输入与神经网络的输出类别不同时，网络将会使用梯度下降算法自动调整突触权值：

_w_(_n_1)  _w_(_n_) __( _y_(_n_)  _y_(_n_*) )_x_(_n_)
_j	j	j_
(3.4)


在式（3.4）中，__ 为学习率， _y_(_n_) 是第_n _个输入样本的预测值， _y_(_n_*) 是第_n _个输入样本本身的正确标签，_x_(_n_) 表示为第_n _个输入样本的第 _j _个值。1962 年，Rosenblatt通过研究证明在数据线性可分的情况下，感知器具有绝对收敛性[78]。
_j_
但在 1969 年，Mnisky 和 Paper 发现了感知器仅能解决线性可分数据的分类
问题，而在处理线性不可分的数据时暴露了缺陷[[79]](#_bookmark119)。由于感知器在处理线性不可分数据方面的先天不足，使得人工神经网络研究的发展进入到一个低谷。

      1. BP 神经网络

1982 年，加州理工学院物理学家 Hopfield 提出了一种神经网络模型[80]， Hopfield 神经网络是一种循环神经网络，能有效保证网络在训练过程中向局部极小值收敛，最终达到具有极小值的目标函数。1986 年，Hinton 等人对于多层网络中隐藏层隐藏节点的学习问题提出了通过误差反向传播来解决，即多层前馈的神经网络学习算法，又称之为 BP 算法[81]。由于 BP 是通过输入输出数据的权值决定的，因此对于某个体样本的误差对整体结果的影响可以忽略不记。
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997518268-ae0f923e-0b4b-413e-9818-e75717ec42c1.png#averageHue=%23000000&crop=0&crop=0&crop=1&crop=1&id=t6Qh5&originHeight=49&originWidth=399&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997518957-843cb666-fc92-4c63-80c9-364395c8bf37.png#averageHue=%23000000&crop=0&crop=0&crop=1&crop=1&id=FTOJA&originHeight=49&originWidth=405&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)BP 神经网络的模型结构如图 3.2 所示，该结构可以分为输入层 i、输出层 o以及隐藏层 h 三个部分。输入数据构成的向量集合就是输入层，输入层包括_n _个节点 _x _ (_x_1, _x_2 ,..._x_n ) ， 输入层与隐含层连接权值为 _wih _； 隐藏层包括 _q _个节点 _h _ (_h_1, _h_2 ,..._hq _) ，隐藏层可以是一层或多层，但在 BP 神经网络中一般少于三层，隐藏层的输入是由上一层的输出和权重相运算的点积；输出层最终的结果就是对

20

![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997519518-3bc820fb-c024-49f1-94c7-ada41eae5005.png#averageHue=%23000000&crop=0&crop=0&crop=1&crop=1&id=nNFSJ&originHeight=49&originWidth=316&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)第三章 五通道卷积神经网络与胶囊网络的建立
该样本的最终预期分类的权重值，网络的期望输出就是分类输出的标量，输出层包括_m _个节点 _y _ ( _y_1, _y_2 ,..._y_m ) 。BP 神经网络每一层都会计算与预期结果的误差，并通过反向传播的方式将误差传递到上一层来更新修改上一层的权重。这里规定 BP 神经网络的叙述都在 l 层。
误差反向传播


x1	y1
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997520111-938aeabe-1040-4cff-8663-ec92224a7af6.png#averageHue=%23dfdfdf&crop=0&crop=0&crop=1&crop=1&id=G3hx7&originHeight=37&originWidth=25&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997520533-d6d071a4-c910-4782-b61a-e0499ad21906.png#averageHue=%23dfdfdf&crop=0&crop=0&crop=1&crop=1&id=kONwv&originHeight=37&originWidth=26&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997521124-eee07ef0-c75e-4031-b51a-2a024587022b.png#averageHue=%23ececec&crop=0&crop=0&crop=1&crop=1&id=X4SqZ&originHeight=269&originWidth=608&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
Wih
l
Who
_i_
_k_
_j_
_q_


xi	yi



xn	ym
_i	h	o_
输入层
隐藏层 信息正向传播
输出层


图 3.2 BP 神经网络基础模型结构
对于图 3.2 的 BP 神经网络模型，本文假设 sigmoid 函数是该模型中每个节点的激活函数，其定义式为公式（3.5）。假设输入样本是 _x_1, _x_2 , _x_3..._xn _ ，那么图 3.2中隐藏层每个节点的输出可表示为式（3.6）：
_f _ _x_ 
1
1 _e__x_
(3.5)




_n_
_hj _ _sigmoid _( _xaWih _ _ba _)
_a_1

BP 神经网络每个输出节点的输出可以表示为：
_j _ 1, 2,..., _n_
(3.6)





_s_
_yk _ _sigmoid _(_hzWho _ _bz _)
_z _1
_k _ 1, 2..., _s_
(3.7)

当输入样本数据是  _x_1, _x_2 , _x_3..._xn _ 时， 那么每个样本对应的输出标签为
( _y_' , _y_' ,..., _y_' )，则网络的输出误差根据平方损失函数可以定义为：

1	2	_m_

1 _m_
 2 

_E	y_
_a	a_
_a_1
 _y_' 2

(3.8)


根据梯度下降准则，在（3.8）式中，对于隐藏层输出端的权值和偏置，则有（3.9）：


_W	_ _W_
 __  _E_

(3.9)
_ho	ho_

_Who_






21


_b	_ _b_
 __ _E_

(3.10)



根据链式求导法则有：
_ho	ho_

_bho_

 _E  _  _y_
 _y_'  _y _(1 _y _)_h_

(3.11)

_Who_
_a	a	a	a	j_



 _E  _  _y_

- _y_'  _y_

(1 _y _)

(3.12)

_bho_
_a	a	a	a_


同理，对隐藏层输入端的权值和偏置，更新公式为：


_W  _ _W_
 __ _E_

(3.13)

_ih	ih_

_Wih_




_b  _ _b_
 __ _E_

(3.14)



根据链式求导法则有：
i_h	ih_

_bih_

_E  _  _y_

- _y_'  _y_

(1 _y_
)(1 _h _)_b_

(3.15)

_b	a	a	a	a	j	ih_
_m_
_ih	a_1


_E  _  _y_

- _y_'  _y_

(1 _y_
)(1 _h _)_b_

(3.16)

_b	a	a	a	a	j	ih_
_m_
_ih	a_1

BP 神经网络算法可以被认为是最成功的神经网络学习算法，其本质就是一个 “误差修正函数+网络模型”，并在训练过程中不断根据模型预想的结果来进行误差矫正，从而进一步更新网络中的权值和偏置，使得模型的输出与预期的结果相一致，BP 神经网络良好的非线性性能使其在目标分类、预测等任务上有良好的表现。1989 年，Robert 认为连续函数在任何闭区间内，BP 神经网络都可以近似逼近，该结论又称之为多层感知器的万能逼近定理[[82]](#_bookmark122)。
BP 神经网络良好的非线性映射能力引起了越来越多研究者的兴趣。但在1991年，BP 神经网络算法在训练过程中暴露出“梯度消失”的问题。误差在反向传递的过程中，梯度值由后面各层的梯度以乘性的方式叠加后再传递到前面的一层，使得神经网络前面层的误差梯度接近零，导致神经网络前面层无法通过权值更新的方法达到学习的目的，同时在网络没有附加动量因子时导致网络容易陷入到局部最优状态中。除此之外，20 世纪 90 年代末，以决策树学习[83]，支持向量机[84]，随机森林[[85]](#_bookmark125)为代表的基于概率论与数理统计的多种浅层学习算法不断崛起，与之前的神经网络相比不仅在分类、回归等任务上均取得了更好的效果，而且基于统

22

第三章 五通道卷积神经网络与胶囊网络的建立
计学习方法的原理相对简单并与神经网络存在显著的差异，使得神经网络的发展又一次进入了瓶颈期。

      1. 深度神经网络

Hinton 在 2006 年提出的深度信念网络（Deep Belief Network，DBN）中[86]，该网络主要由受限玻尔兹曼机组成（Restricted Boltzmann Machine，RBM）[[87]](#_bookmark127)，可以看成若干个 RBM 堆叠而成，深度信念网络基本结构如图 3.3 所示，受限玻尔兹曼机与玻尔兹曼机基本结构如图 3.4 所示。在对神经网络的每一层进行无监督训练时，将网络中的每一层都训练完成之后利用前面的 BP 算法对整个网络进行迭代训练，并利用梯度下降更新网络的权值，这种训练方法称之为无监督逐层训练，随着硬件算力的提升、云计算等技术手段的到来大幅度降低了训练难度，此外数据样本的爆发增长，使得神经网络有足够的样本用于训练。从此神经网络又得到了新的发展，深度信念网络、卷积神经网络[[67]](#_bookmark107)、长短期记忆神经网络[87]在图像分类、目标检测等领域取得成就令人称赞，从那之后，以深度神经网络为代表的复杂神经网络模型开始受到人们的关注。
玻尔兹曼机中的神经元通常只有 0、1 两种状态，激活状态用 1 表示，抑制状态用 0 表示。在图 3.4（a）中，令_n _个神经元的状态用向量_s _0,1_n _表示，__ 表示神经元 _i _与 _j _之间的连接权，则向量 _s _对应的玻尔兹曼机的能量可定义为式
_ij_
（3.17），当网络中的神经元在任何条件下都不依赖于输入值的顺序而进行更新时，这时状态向量 _s _仅由其本身的能量和状态向量的能量确定，如式（3.18）。
_n_1  _n	n_
_E_(_s_)    _ijsis j _ _isi_
(3.17)
_i_1 _j __i_1	_i_1


_e_ _E _(_s_) 
_P_(_s_) 	_e_ _E _(_t _) 

_t_

(3.18)
图 3.4（b）的受限波尔兹曼机常通过“对比散度”（Contrastive Divergence， CD）算法来完成网络的训练[88]。首先假定该神经网络中的显层有_d _个神经元，_q_个隐层神经元，显层与隐层的状态向量分别由 _v _和_h _来表示，那么在所有给定的隐层中，显元的取值有：


_d_
_P_(_v _| _h_)   _P_(_vi _| _h_)
_i_1

同理，在给定的显层中，隐元的取值有：

(3.19)





23


_q_
_P_(_h _| _v_)   _P_(_hj _| _v_)
_j _1

(3.20)


其中， _h _ _h_1, _h_2 ,..., _hq _ ， _v _ _v_1, _v_2 ,..., _vd _ 。对于每个训练样本_v _，CD 算法首先根
据公式（3.20）算出隐层神经元的概率分布后，在该概率分布上采样得到 _h _，同理根据公式（3.18），从_h _产生_v_' ，再从_v_' 产生_h_' ，那么连接权的更新公式为：

__  __ _vhT _ _v_'_h_'_T _
(3.21)


RBM	RBM	RBM	RBM
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997522060-5f1ee4fc-b221-4d75-b115-c93e489e9b7c.png#averageHue=%23e3e1e1&crop=0&crop=0&crop=1&crop=1&id=w64sX&originHeight=425&originWidth=695&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
输入层	隐藏层1	隐藏层2	隐藏层3	输出层

图 3.3 深度信念网络基本结构

![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997522569-d183f6a0-35e3-4af9-b08a-c949af5d52f9.png#averageHue=%23d9d9d9&crop=0&crop=0&crop=1&crop=1&id=ChAYd&originHeight=183&originWidth=282&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997523106-c14dbadf-531b-4df5-874d-72f7ee5b4458.png#averageHue=%23d5d5d5&crop=0&crop=0&crop=1&crop=1&id=Rhw51&originHeight=217&originWidth=282&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)隐层



显层

         1. 玻尔兹曼机	(b)受限玻尔兹曼机

图 3.4 兹曼机基本结构图: (a) 玻尔兹曼机; (b) 受限玻尔兹曼机
在单个 RBM 完成训练之后，显层中显元与隐层中隐元之间的权重矩阵 _W _为：
 _w_11	_w_21	...	_wd_1 
_w	w	_...	_w  _
_W _ 
12	22
_d _2 
(3.22)

 ...	...	...	... 
_w	w	_...	_w  _
 1_q	_2_q	dq _
在式（3.22）中， _wij _表示从第_i _个显元到第个 _j _隐元的权重。对于一个样本数据
_x _ (_x _, _x _,..., _x _)_T _输入到显层中时，RBM 会根据权值来选择是否开启或者关闭隐
1	2	_d_

24

第三章 五通道卷积神经网络与胶囊网络的建立


元，计算每个隐元的激励有：

_h _ _Wx_

(3.23)

对于隐层的每个隐元使用激励函数 Sigmoid 进行标准化后，则可得到这些隐元处于开启状态的概率值：

_P_(_h_
_j _ 1)  __ (_hj _) 
1
1 _e__hj_
(3.24)
此时可得到隐层中每个隐元的开启概率。那么这些隐元处于关闭状态的概率为：

_e__hj_

_P_(_hj _ 0)  1 _P_(_hj _ 1) 
1 _e__hj_
(3.25)

通过将开启的概率 _P_(_hj _ 1) 与一个从 0 至 1 均匀分布中抽取随机值_u _进行比较来决定隐层的隐元到底是开启还是关闭，其中：
_u __U _(0,1)	(3.26)

_h _ 1,
_P_(_hj _ 1)  _u_

(3.27)
_i	_0,

_P_(_hj _ 1)  _u_


同样，对于给定的隐层计算显层的方法同理。
对 RBM 进行训练时，其本质是得到一个最能代表训练样本的一个概率分布。另一方面，由于权值_W _对该分布有着决定性作用，因此训练 RBM 的最终目标就是找到一个最佳值，这就是 CD 算法的本质目的。将训练集中的样本 _x _输入到显层_v_(0) 中，那么通过计算隐层的神经元激活的概率为：

_P_(_h_(0)  1| _v_(0) )  __ (_W v_(0) )
_j	j_
随后从得到的概率中抽取一个样本：
_h_(0)  _P_(_h_(0) | _v_(0) )
(3.28)


(3.29)



用_h_(0) 重构显层，即：

_P_(_v_(1)  1| _h_(0) )  __ (_WT h_(0) )
_i	i_


(3.30)



同理，从显层概率中抽取一个样本：
_v_(1)  _P_(_v_(1) | _h_(0) )


(3.31)


对于显层样本的结果，再次对显层的神经元重构，那么可得到隐层神经元被开启的概率为：

_P_(_h_(1)  1| _v_(1) )  __ _W v_(1) 
_j	j_
(3.32)


最后 RBM 的权重更新为：
_W _ _W _ __(_P_(_h_(0)  1| _v_(0) )_v_(0)_T _ _P_(_h_(1)  1| _v_(1) )_v_(1)_T _)


(3.33)


25

在图 3.3 中的每一椭圆框都是一个 RBM，多个 RBM 共同组成了 DBN。在网络的训练过程中，首先对 RBM 完成无监督训练，也就是“预训练”，RBM 最大的优势在于即使样本的分布概率在未知的情况下，RBM 都能将其转换为状态向量的能量模型。但由于 DBN 在训练过程中是对每一层 RBM 分别进行训练，训练结束后的结果可能不是全局最优，因此会再次对网络模型进行微调。“预训练”加上 “微调”的方式能有效克服神经网络带来的梯度消失问题。
2012  年，Hinton  等人通过卷积神经网络训练的深度学习模型（AlexNet）在 ImageNet  图像分类上刷新了有史以来最好的成绩且超过了传统统计学的分类方法（SVM）并取得了第一名[67]。同年 11  月，微软研发的全自动同声传译系统在天津的一次参展活动中运用深度学习的方式实现了英中机器翻译和中文语音合成，该系统背后的主要原理也是深度学习。鉴于卷积神经网络学习在图像识别中的成功应用，随后相继出现了 VGG[[71]](#_bookmark111)、GoogLeNet[89]以及 ResNet[90]等神经网络模型。在 2016 年，DeepMind 公司针对为期比赛开发了基于深度学习的 AlphaGo系统[25]。随着深度学习研究的深入，其应用场景开始在多方面取得进展，如语音识别、图像分类、人脸识别及目标检测等。本文以卷积神经网络为深度学习模型进行微表情识别分类，接下来主要介绍卷积神经网络的基本原理。
#### 0.8 卷积神经网络的基本原理
1987 年，Alexander Waibel 等人针对语音识别问题提出了时间延迟网络（Time Delay Neural Network，TDNN）[[91]](#_bookmark131)。1988 年，Wei Zhang 等人设计了第一个二维卷积网络即平移不变人工神经网络（SIANN）并应用于检测医学影像[92]。1989年，Yann LeCun 等人设计了 LeNet 网络模型[93]，在其论文中第一次提到了“卷积”一词，从此“卷积神经网络”这一名称也就诞生了。1998 年，在 LeNet 的基础上 Yann  LeCun 等人针对手写数字识别问题设计了卷积神经网络 LeNet-5，并得到了成功的应用[[94]](#_bookmark134)。随着卷积神经网络研究的发展，基于卷积神经网络的应用领域也越来越多，在语音识别、图像处理、机器翻译等场景中取得了更好的效果。

      1. 局部感知与权值共享

在传统的卷积神经网络中，全连接通过前馈神经网络返回误差有个致命的缺点，当数据规模太大时会导致参数巨大难以训练。而在目前的卷积神经网络中通过两个重要的方法来减少网络训练时的权重参数，即权值共享与局部感知又称为稀疏连接与参数共享。
权值共享中每个神经元使用同一个卷积核对图像做卷积，其本质就是一个全图滤波。对于局部感知，由于人们的认知是从局部到全局，而图像中目标的空间

26

第三章 五通道卷积神经网络与胶囊网络的建立
联系是距离远的像素之间相关性较弱，距离近的像素之间的相关性较强。因此将局部的感知信息综合起来就可得到图像的全局信息。如图 3.5 所示为权值共享与
局部感知示意图，假设图 3.5 大小为 1000×1000，当采用 3×3 作为局部感知区域，
此时需要 106×9 个权值参数。卷积神经网络中有另一种连接方式如图 3.6 的全连接方式，当图像大小相同时，隐藏层神经元的数量为 106，若使用全连接的方式那么网络总的参数个数将达到 1012。由此可见，在卷积神经网络中接入局部感知时，若隐藏层的 106 个神经元对应的 9 个参数是相同的，那么权值的参数量将减少为 9。该方式与图像中目标所在的位置没有关联，因为图像中一部分的统计特征与其他部分是一样的。

![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997523598-c0f66822-78db-46a2-8c3d-4391594ac19f.png#crop=0&crop=0&crop=1&crop=1&id=mkhuP&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)

图 3.5 权值共享与局部感知
图 3.6 卷积神经网络中的全连接

      1. 卷积层与池化层

![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997524047-5d2c315b-62e3-497c-a905-93537913149b.png#crop=0&crop=0&crop=1&crop=1&id=FmjK6&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997524579-acc0a082-b3f9-4888-a7fe-d758a9795e1d.png#crop=0&crop=0&crop=1&crop=1&id=jW80Y&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997524868-961873fc-05a9-4bbe-9204-b95457431179.png#crop=0&crop=0&crop=1&crop=1&id=si1Si&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997525188-a5b78733-e0fd-4f2e-998e-23f1dce66ef4.png#crop=0&crop=0&crop=1&crop=1&id=t4ZEz&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)卷积神经网络由多个卷积和池化单元组成，而卷积的参数都是通过 BP 算法训练得到最佳参数进而更新权重和偏置。基本的卷积神经网络结构如图 3.7 所示。
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997525664-e3cafe16-3e1e-49fc-adc5-3b68831d28d3.png#crop=0&crop=0&crop=1&crop=1&id=EaGo5&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997526151-c0403186-59c6-44f5-bda7-749c27b275d7.png#crop=0&crop=0&crop=1&crop=1&id=uDhLM&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997526681-ede434b5-3c2b-46a3-b7ac-cda8bdf3a9c1.png#crop=0&crop=0&crop=1&crop=1&id=jx30I&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997526951-4c475d61-0191-4950-8344-ddb44a6f55ec.png#crop=0&crop=0&crop=1&crop=1&id=pkeqp&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)


![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997527293-50a76261-3b7d-462f-abb0-bec4fe8a0c8a.png#crop=0&crop=0&crop=1&crop=1&id=TtLcN&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997527844-18dedc04-c827-48f7-b074-f0b3731fe105.png#crop=0&crop=0&crop=1&crop=1&id=Z4uWU&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997528218-7619c210-13aa-434b-a03c-97c4337d4351.png#crop=0&crop=0&crop=1&crop=1&id=LJiVJ&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997528758-f7d144b4-30f0-43b1-ad91-d073f0ce2519.png#crop=0&crop=0&crop=1&crop=1&id=mvIhk&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)图 3.7 卷积神经网络基本结构
如图 3.7 所示，该结构多使用局部感知（稀疏连接）即卷积核和权值（参数）

27
共享方式以提取网络中输入图像的特征信息。在卷积神经网络中，通常每个卷积层包含有多个卷积核，以进一步保证提取到的特征能够充分表征上层图像。这里
假设来自上层的特征图像为_x _，像素为_m__n _，那么卷积层的输出_C j _表示为：

_C j _
_f _(_wj x _ _b j _)
(3.34)

其中，当有多个卷积核时_wj _表示卷积核中的第 _j _个卷积核， _b j _表示第 _j _个卷积核的偏置， _f _表示神经元的激活函数， _C j _表示第 _j _个卷积输出特征图。
在神经网络中，激活函数对网络的性能非常重要，尤其对于学习、理解非常复杂和非线性的输入可以更进一步增加神经网络对于特征信息的非线性映射能力。常用在神经元的激活函数有 Relu、Tanh 以及 Sigmoid。
池化层介于卷积层之间主要用于数据压缩和减小参数的数量，提高模型的泛化能力。池化层常用的方法有最大池化和平均池化，如图 3.11 所示，和卷积核类似，最大池化是根据记录的最大值来保留一个特征矩阵，随后根据该特征矩阵来提取最大值；而平均池化就是针对每一个像素值相加之后再取平均值最后生成一个特征矩阵。假设池化层的特征输出图是_S j _，那么有：
_S j _ _f ___ _jdown __C j _  _b j _
(3.35)

这里__ _j _与_b j _分别表示池化层中第 _j _个输出的神经元的乘性因子和偏置，_C j _表示来自卷积层的第 _j _个输出特征图， _down_() 是池化层中的一个下采样函数。通常情况
下下采样函数包括最大池化（3.36）和平均池化（3.37）。

_down __C j_
_R__R	kt_
  max _C j _
(3.36)

_down __C j_
_R__R_
	_mean _ 

_kt _
_R_
_C j_

(3.37)

 _k _1 _t _1	
	 _R_
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997529192-15a40d38-f6be-4693-b3ca-61a4620a9139.png#crop=0&crop=0&crop=1&crop=1&id=M0lRM&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
图 3.8 最大池化与平均池化

28

第三章 五通道卷积神经网络与胶囊网络的建立
式（3.36）和（3.37）中， _C j  _表示来自卷积层的特征图像_C j _大小为_R _ _R _的区域，
_R__R _
这里_k _和_t _的取值范围在1, 2..._R _。此外，池化层仅仅做的是图像压缩和减少参数，不需要对池化层设置额外的学习参数，需要做的是确定池化层的采样函数类型、池化核大小等参数。
全连接层能够有效保留目标类别区分度的局部特征信息。此外，全连接层神经元的激活函数一般使用 Relu，同时，为了克服神经网络在训练过程中出现过拟合的状况，一般会在卷积神经网络的全连接层使用 Dropout 技术，使得网络在一定程度上达到正则化效果[[95]](#_bookmark135)。标准的全连接神经网络和应用 Dropout 的神经网络如图 3.9 所示。L2 正则化以及其它约束参数的技术常应用在 Dropout 中，网络应
用 Dropout 之前如式（3.38）变为（3.39），由（3.39），使用 Dropout 之后，学习率由因子_q _ 1 _q _进行缩放，而__ 和_q _之间的范围变化为式（3.40）。
_w _ _w ___(_f _(_W _; _x_)  _w_)
_w_
(3.38)
_w _ _w ___(  1
_f _(_W _; _x_)  _w_)
(3.39)
1 _p	__w_
_r_(_q_)  __  __  lim _r __q_,   lim _r_(_q_)
(3.40)

_q	_
_q_1
_q_0	

![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997529570-00ae735b-972c-42e6-a0e3-a896178a22c6.png#crop=0&crop=0&crop=1&crop=1&id=tIycK&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997530367-b6305f5e-e91e-45eb-b0e2-873685584350.png#crop=0&crop=0&crop=1&crop=1&id=cUSvr&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)其中， _q _表示推动因子，由于具有增强学习速率的作用，故_r_(_q_) 被称为有效的学习速率。利用 Dropout 方式降低了神经元节点之间的相互依赖性，使得特征之间的独立性和特征的表征性更具有一般性，因此卷积神经网络模型泛化性能更具优势。
标准神经网络	应用Dropout后

图 3.9 标准神经网络与应用 Dropout 后的神经网络



29

      1. Softmax 分类



Softmax 逻辑回归由 logistic 回归模型在多分类问题上推广而来，多分类问题中目标的输出标签取值均大于两个。Softmax 回归是有监督的训练学习过程，卷积神经网络在训练中通过反向传播误差来更新网络前面的权重和偏置，与前面章节中介绍的 BP 神经网络类似，但区别在于 BP 神经网络中的误差使用的是平方损失函数，而现今在卷积神经网络中多分类输出采用 Softmax。
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997531032-7b255c52-7d68-4d0a-b16f-f01ab9836016.png#crop=0&crop=0&crop=1&crop=1&id=pmhGu&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)假设训练集的样本有_m _个[(_x_(1) , _y_(1) ), (_x_(2) , _y_(2) ),..., (_x_(_i_) , _y_(_i_) ),..., ( _x_(_m_), _y_(_m_) )] ，其中，第_i _个训练样本为 _n _1 维的列向量用 _x__i_ 表示； _y__i_ 是该样本所对应的真实标签， _y__i_ 1，2， ，_k_，那么样本一共为_k _类。将样本 _x _的特征输入到神经网络的分类
器中经过计算预测该样本属于某个类别的概率值，即 _P _ _y _ _j x_ ，如式（3.41）：

 _p _ _y__i_  1 _x__i_ ,__  

__ _T x__i_

	1	
_e _1	

 _p  y__i_  2 _x__i_ ,__	
	
	1		__ _T x__i_

_h __x__i_    
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997531535-bce02300-746c-4265-bf40-deba4cf6f87a.png#crop=0&crop=0&crop=1&crop=1&id=D7S7Q&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)

__

2 


_e _2	

![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997532066-78114d97-e874-406f-bfdd-13a954365033.png#crop=0&crop=0&crop=1&crop=1&id=Z34jT&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)



_k_
_e_ _T x__i_ 
_j_
(3.41)
	_i_
_i_
	_j _1
_e_ _T x__i_ 

 _p _ _y_
 _k x_
,_k_

  _k	_


其中，_k _为_n _1的列向量，代表网络中第 _j _个节点的输出与全连接层之间的连接权值。为了训练模型参数_k _，Softmax 的代价函数为式（3.42）所示，使得能够最小化代价函数。其中，1[y(_i_) =j] 为示性函数，当第_i _个样本的真实标签与 _j _相等时值为 1，反之其值为 0。因此，代价函数的本质就是通过调整参量使得样本的真实类别与网络中预测的对应概率值不断变大，该方式能有效使得负对数似然函数的
值趋近于 0。
_J ___     1 _m __k  _1 _y__i_  _j_  log _p __y__i_  _j x__i_;__ 	(3.42)
_m _
_i_1
_j _1 		

其中： _p _ _y__i_ 
_j x__i_;__  
exp__ _T x_

，将该函数展开可得：
_l_

_k_
_i_

_l _1
_l_
exp__ _T x_

_J ___     1 _m __k_
1_y__i_  _j___ _T x__i_  log_k_
_e_ _T x__i_ 

(3.43)

_m _
_i_1
_j _1
_j	l _1	

在实际应用中，常利用 SGD 算法来优化代价函数不断迭代达到最小值，并使得该值达到全局最优，通过对误差函数求得偏导数后接着利用式（3.44）更新参数值。

__ _j _: __ _j_
__ _J ___ 
__

(3.44)

_j_

由 _J ___  对__ _j _求偏导：

30

第三章 五通道卷积神经网络与胶囊网络的建立



_J ___    1
_m _1_y__i_ 
_j__x__i_ 
_x__i__e_ _T x__i_  

(3.45)

__ _j_
_j_
_l_

_m __i_1 

_k_

_l _1
_e_ _T x__i_  

利用 BP 算法结合 Softmax 以及随机梯度下降对网络中各项参数进行迭代优化。

#### 0.9 胶囊网络
卷积神经网络（Convolutional Neural Networks, CNN）能较好的完成图像识别、分类等任务，在完成这些任务的过程中起决定性作用的就是卷积核。卷积核可以通俗的理解为图像的滤波器（Filter），其主要功能就是把输入图像中的所有像素与滤波器中的数值做相应运算，其输出图像作为特征矩阵。通过对输入图像进行多层的卷积、池化后提取到图像的高级特征来表征原始图像。但 Hinton 对 CNN中的池化功能发出了质疑，CNN 在进行最大池化或平均池化时存在丢失图像的特征信息缺点，同时 2012 年胶囊网络（Capsule Network）的概念由此应运而生[96]。

      1. 胶囊网络的总体结构

基础胶囊网络结构如图 3.10 所示，该网络由简单的三层 CapsNet 构成。当直接将预处理后的原始数据输入到网络中时，一般会通过一层卷积神经网络来提取图像的低水平特征，这正是卷积神经网络的优势所在，同时为了保证提取到的图像特征信息尽量完整，因此不在该网络中添加池化层，接着该胶囊网络结构会将第一层卷积提取到的低级特征通过 PrimaryCaps 层做动态路由运算，此时原始特征信息从单一的“像素”值向矢量特征开始转换，在 Hinton 的著作《Dynamic Routing Between Capsules》[96]中 DigitCaps 层的胶囊数量为 10 且维度是 16。当需要预测输入样本属于某一个类别的时候先分别求出这 10 个胶囊中向量的模长，此时求得向量模长最大值的胶囊便对应着输入样本的预测类别。通过这种计算方法来判断目标是否存在明显的特征，若向量模长的大小决定目标的概率。
DigitCaps




![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997532566-7d81c9d4-212b-45c3-a5e0-00170b9031e2.png#crop=0&crop=0&crop=1&crop=1&id=hUE6a&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)10
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997533042-01213f37-a9fb-476d-b0ab-264b6d26e61b.png#crop=0&crop=0&crop=1&crop=1&id=vbjJe&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997533598-f8c0251a-bef8-483c-b964-d6c4908f3ca3.png#crop=0&crop=0&crop=1&crop=1&id=ycAIU&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997534100-9cb34746-2fae-4f16-9aaf-5da30d2f0f05.png#crop=0&crop=0&crop=1&crop=1&id=mX3Sc&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
PrimaryCaps
Conv1
9×9
9×9
...







16

图 3.10 基础胶囊网络结构
31

      1. 胶囊与神经元的区别



传统的深度神经网络如卷积神经网络（CNN）、循环神经网络（RNN）与胶囊网络有着很大的区别，如表 3.1。胶囊网络内部将一组传统的神经元向量化之后，那么目标存在或者目标的一小部分存在的概率由该向量的长度来表示，而目标的属性如方向、大小、颜色、位置、形状等则由向量的方向来描述。通过胶囊输出一个同模长而方向不同的特征向量来表示图像中目标轻微的变化，如方向、大小、位移等。因此当网络的输入有任何变化时，胶囊会立刻映射到输出结果上给定目标的变化信息，即输入和输出是等变化的。胶囊网络的另一个特点在于使用了动态路由机制，该机制使得胶囊网络通过迭代的方式来计算输出结果。总的来说，胶囊网络通过胶囊结构来学习和检测给定目标的特征信息，在传统的神经网络中神经元仅仅能检测目标特征是否存在，而对于胶囊的输出向量不但可以表示目标的特征信息，而且还能存储和传递目标特征的各种属性。
从表 3.1 可知，CNN 中的输入标量 _xi _由胶囊网络的_d _维向量_ui _进行了替换以

此来存储和表达输入信号的特征。胶囊网络实现底层特征_ui _到高阶特征_u j_|_i _的映射
	
变换使用特征变换矩阵_u j_|_i _ _Wijui _，而 _s j _和_u j_|_i _两者之间的连接强度是通过度量聚

类中心_sj _和_u j_|_i _之间的相似度_cij _来实现，高阶特征通过这种方式能有效实现底层特
征的动态聚类。胶囊网络中的压缩函数使用表 3.1 中_ui _的非线性激活函数，该非线性激活函数的主要作用在于能够将有界指标衡量，来比较样本各特征的显著程

度，即将前面聚类中心 _s j _和_u j_|_i _的向量模长压缩到 0-1 之间，而样本的某一特征的
显著程度主要用向量的模长来表示，预测样本的归属类别概率以各向量的模长

_vj _来表示。


表 3.1 胶囊与神经元的主要区别



神经元 VS 胶囊
输入	_xi	ui_

加权求和
_aj _ _Wi xi _ _b_
_i_
_s j_
2
_j_
2
_s j _ _cij u j_|_i i_


_h	_(_x_)  _f _(_a _)
_v _	. _s j_

运算	非线性激活函数
_w_,_b	j_
1 _s j_




矩阵变换	——
_s j_

_u j_|_i _ _Wijui_


输出	scalar( _h _)	vector( _vi _)



32

第三章 五通道卷积神经网络与胶囊网络的建立

      1. 胶囊网络中的姿态矩阵

数学意义上的姿态矩阵（Pose Matrix）可以用来表示图像中物体的缩放、平移以及旋转等变化。假设在一个平面直角坐标系中，若 _X _(_x_, _y_) 为原坐标点那么有如下变换，按照顺序将 _X _平移三个单位后再将 _X _放大一倍，随后 _X _继续逆时针旋转 60 度，变换后的坐标点为 _X _(_x_', _y_' ) ，经过缩放、平移等操作后的矩阵可表示为式（3.46），式中等号的右边，从左到右的三个矩阵依次为平移变换矩阵、缩放变换矩阵以及旋转变换矩阵。
 _x _	1	0	3 2	0	0 cos 60	sin 60	0  _x_' 
 _y_  0	1	0 0	2	0 sin 60	cos 60	0  _y_' 
 		 	 	 	
(3.46)

1
0	0	1 0	0	1 
0	0	1  1 


将图像中目标的各个部分与子部分之间的相对位置、角度等关系以姿态矩阵的方式来存储，以此方便后期对网络的训练和对输入图像的识别与分类，是胶囊网络最大的特点之一。众所周知，人由脸和身体等其它部分组成，而脸的组成部分包括鼻子、眼睛以及眉毛等，身体的组成部分包括骨架、肩膀以及手等，故人作为一个整体此时由三个不同的单元组成，如图 3.11 所示。若输入图像中存在一个人作为目标时，那么人的每一个组成部分都可以通过姿态矩阵进而互相联系。总的来说，当人作为一个整体对象时，我们从不同的角度将会看到不同视角的人，但不可改变的是，人每个部分之间的相对位置总的来说大致相同，因此推测人脸在图像中的相对位置可以通过姿态矩阵中嘴、眉毛、鼻子等部分的位置向量值来判断。
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997534716-eb208eee-4366-4ecc-bfc2-c8d0f40c505a.png#crop=0&crop=0&crop=1&crop=1&id=bhMaG&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
身体
脸
肩膀	小腿
脚
手
嘴
眉毛	鼻子	眼睛

图 3.11 人的整体与部分层级关系




33

      1. 动态路由算法



胶囊网络的结构中包括很多层，每层又包括多个节点，每个节点中的标量神经元又称之为胶囊。在图 3.13 中，PrimaryCaps 和 DigitCaps 层之间的链接运行机制称之为动态路由，不同于卷积神经网络中的全连接，在胶囊网络训练过程中通过不断迭代更新网络权值进一步引起节点之间连接强度发生变化[[97]](#_bookmark137)，PrimaryCaps的输出结果会生成一个预测向量，如式（3.47），随后通过与对应的耦合系数加权求和如式（3.48）：

_u j_|_i _ _uiWij_

(3.47)



_s j _ _u j_|_i cij_

_i_
(3.48)


其中_cij _，通过 softmax 回归函数来决定耦合稀疏_cij _。如式（3.49）满足_cij _累加和为 1。在更新参数时，开始阶段将_bij _初始化为 0，此时后一层胶囊和前一层胶囊之间的关系并不能决定，因此需要通过更新参数_bij _来进一步更新耦合系数_cij _，_bij_更新表达式如下：
_cij _ _soft _max(_bij _) 


_bij _ _bij _._v j_
_ebij_


_n_
_ebij_
_i_1

(3.49)



(3.50)




![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997535161-37f3a522-7f75-466c-8180-da21aed394a8.png#crop=0&crop=0&crop=1&crop=1&id=c3wZC&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
1
k
squash
squash
1
k
迭代r次
11	N1	1k	Nk
1|1	 
1|N
k|1	 
k|N
11
1k
N1
Nk
1
 
N

图 3.12 动态路由算法过程
这里的_bij _由上一层_bij _和后一层的胶囊输出结果_vi _点积共同决定。在文献中给胶囊网络设计了一个最新的压缩函数（Squashing Function），也就是传统神经网络中的

34

第三章 五通道卷积神经网络与胶囊网络的建立
激活函数。根据压缩函数可得胶囊输出向量_vj _的计算结果如式（3.51）。


_v j _
_s j_
1 _s_
2
_s j_
2
_j_
_s_
_j_
(3.51)

胶囊网络的动态路由过程如图 3.12 所示。_ui _和_vj _分别是 PrimaryCaps 中的向量胶囊和 DigitalCaps 中的胶囊。事实上_ui _的聚类中心可以看成是_vj _，那么动态路由的本质就是通过对输入信息提取特征后进行特征整合，实现 PrimaryCaps 到 DigitalCaps 的非线性映射。
#### 0.10 五通道卷积神经网络和胶囊网络模型的构建
微表情是人脸局部区域的微小运动其出现的持续时间大约在 1/25s~1/5s。由于微表情运动强度低、持续时间短等特点，捕捉识别微表情具有很大的困难。1996年微表情被发现以来一直受到人们的关注[98]，以往大部分研究人员要么是从微表情序列的顶帧来提取面部微表情特征，要么是从整个人脸中来提取面部微表情，要么是从嘴巴、鼻子等局部区域来提取微表情特征。但微表情在时间上是一个连续运动的过程，因此分析考虑面部区域运动过程对微表情识别的影响其理论意义和实际意义将会更有助于改善微表情识别精度。
微表情是人脸局部区域的运动过程，包括面部区域运动开始阶段、运动峰值阶段以及运动结束阶段，有效捕捉面部区域的运动趋势对识别微表情很有帮助，根据光流（Optical Flow）算法理论可以发现面部区域局部像素的运动强度和方向。因此本文使用开始帧和顶帧的水平光流和垂直光流作为其中两个通道的输入，求得顶帧和结束帧的水平光流和垂直光流作为另外两个通道的输入，其余一个通道输入微表情顶帧的灰度图像。随后将五个通道提取到的微表情特征进行融合之后作为胶囊网络的输入来进一步对微表情特征进行增强，由于胶囊网络保存的是目标的相对关系等信息，因此输出一个特征向量可以为后面的分类提供更多有区分性的特征。因此本文通过建立包含胶囊网络模块的五通道卷积神经网络来完成微表情识别分类任务。

      1. 五通道卷积神经网络

如图 3.13 所示五通道卷积神经网络结构图，该结构通过前面所介绍的池化层和卷积层共同组成。从第二章中所介绍的两个数据集可以看出，实验可用的有效样本较少，总的加起来也不足 500 个。虽然深度神经网络理论上会随着网络深度的增加，同时识别精度会跟着增加，另外相关研究表明层数越深的神经网络并不适用于在小样本数据集上训练[[26, ](#_bookmark66)[99-101]](#_bookmark139)，同时随之而来的问题是深度学习参数多、

35

结构复杂、对数据量要求比较大，因此本文使用浅层的卷积神经网络完成微表情特征提取。

![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997535700-40114e9d-37ed-4636-985d-dac379eb6f0c.png#crop=0&crop=0&crop=1&crop=1&id=W2YDY&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)Input 224*224
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997536215-2e25b0b6-c6d1-4ffc-aab3-cfb41816958b.png#crop=0&crop=0&crop=1&crop=1&id=wPC7M&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997536793-3c896c15-bd8b-4ec1-ace4-549048430969.png#crop=0&crop=0&crop=1&crop=1&id=vKmbM&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
1
1
64
Conv1
Pool1
Conv2
Pool2
Conv3
Pool3






On_Ap_V
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997537263-9de4b0bb-c5ca-4f33-8504-819c7aea9d92.png#crop=0&crop=0&crop=1&crop=1&id=neK7s&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997537649-cee0c9e2-b3e0-4150-9e5f-c2da860cf669.png#crop=0&crop=0&crop=1&crop=1&id=dHGyW&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997537969-252fd291-16f0-4701-a16b-53786d086237.png#crop=0&crop=0&crop=1&crop=1&id=FJiOr&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997538325-05825294-c87b-4e76-8281-053373851221.png#crop=0&crop=0&crop=1&crop=1&id=LFNsI&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997538880-bca94abc-288d-48a8-9650-a5aef47b38e5.png#crop=0&crop=0&crop=1&crop=1&id=lgUfQ&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)

![](https://cdn.nlark.com/yuque/0/2022/jpeg/22897659/1670997539370-e12269dc-a587-49b3-8c12-9a8906ce84f8.jpeg#crop=0&crop=0&crop=1&crop=1&id=p6AQk&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
On_Ap_H

![](https://cdn.nlark.com/yuque/0/2022/jpeg/22897659/1670997540442-e0819d64-c046-4635-8766-a20a747d6254.jpeg#crop=0&crop=0&crop=1&crop=1&id=xseYx&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
Apex_Gray

![](https://cdn.nlark.com/yuque/0/2022/jpeg/22897659/1670997540962-ead3f905-3a2b-42af-9d01-4b9d731172cb.jpeg#crop=0&crop=0&crop=1&crop=1&id=FLavZ&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
Ap_Of_V

![](https://cdn.nlark.com/yuque/0/2022/jpeg/22897659/1670997541451-0f08c622-428d-4a46-884e-e713b7221642.jpeg#crop=0&crop=0&crop=1&crop=1&id=NrPGr&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
Ap_Of_H

16	16	32	32	64
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997541973-8d16de26-033c-41b4-b239-43657fce8dc7.png#crop=0&crop=0&crop=1&crop=1&id=WFL9q&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)


5
5
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997542555-88394dcd-eb51-4e8b-b455-560f3f9a6def.png#crop=0&crop=0&crop=1&crop=1&id=m4njB&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)


1
1
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997543121-9b105c38-384e-40e8-95d2-75305aa3e7c5.png#crop=0&crop=0&crop=1&crop=1&id=DBIwx&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)

3
3
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997543536-359d837b-add5-41c9-bdb0-3c135674b050.png#crop=0&crop=0&crop=1&crop=1&id=W3bVd&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)

1
1
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997544062-0f9077de-5211-4be7-8a4d-20d377257334.png#crop=0&crop=0&crop=1&crop=1&id=dFPnt&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)

3
3

![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997544571-4d4e143b-9eb6-493f-8851-4677428e92e2.png#crop=0&crop=0&crop=1&crop=1&id=PtZDn&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997545039-fbc78692-a878-4ed3-8f0c-3f8b5f2454e2.png#crop=0&crop=0&crop=1&crop=1&id=Ghwju&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997545320-8e8c7795-2723-49e0-8f87-fa144f9f0569.png#crop=0&crop=0&crop=1&crop=1&id=HSesq&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997545693-8ab4a589-ef11-40b0-9f3c-97587b89891c.png#crop=0&crop=0&crop=1&crop=1&id=oc6Yk&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997546212-c29a4d30-83c1-45d0-abf4-efa7098d3c59.png#crop=0&crop=0&crop=1&crop=1&id=UP9qo&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997546795-fd8870b2-51bb-4300-84ca-e2a1035d3c0c.png#crop=0&crop=0&crop=1&crop=1&id=mO23z&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997547462-a3ead4ef-5493-4cc2-b12f-5b5f93a47311.png#crop=0&crop=0&crop=1&crop=1&id=WOzWo&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997547829-2d732b2a-e2c9-4c94-a531-770a641b56cc.png#crop=0&crop=0&crop=1&crop=1&id=yWeDR&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997548367-fc88dcd6-a1ff-4bf3-9dc8-4268ac274ae8.png#crop=0&crop=0&crop=1&crop=1&id=xaenl&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997548775-1553bba1-2db0-437f-9caa-bf91a61a5319.png#crop=0&crop=0&crop=1&crop=1&id=aTWRB&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997549212-4ae9d258-00cb-4c18-b4d9-62d8cf5c8762.png#crop=0&crop=0&crop=1&crop=1&id=hbA0i&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997549874-4268d1ec-5034-4983-9fe8-a8451d159c49.png#crop=0&crop=0&crop=1&crop=1&id=WY0ur&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997550924-286ee466-1423-4ddd-8335-3f55b9a31ae7.png#crop=0&crop=0&crop=1&crop=1&id=QDK4a&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997551457-54ccac0e-6aa4-45db-8aeb-3d42fdf2f080.png#crop=0&crop=0&crop=1&crop=1&id=tDHC3&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997551937-2e687eb8-f5b2-4446-9329-40c9f1c47ce4.png#crop=0&crop=0&crop=1&crop=1&id=jfTvD&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997552587-a0527c02-a563-4b3c-8917-a24b25a3c7bb.png#crop=0&crop=0&crop=1&crop=1&id=xmQMj&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997552991-a9c406a1-4aec-4d37-a6de-bdd2dfd9c9ad.png#crop=0&crop=0&crop=1&crop=1&id=vSrNQ&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997553505-f97e0f6a-f263-47b9-a39d-50f3af43cde8.png#crop=0&crop=0&crop=1&crop=1&id=YRTIz&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997554015-b02c3d78-2cf6-4b33-831d-1bfef50a1d81.png#crop=0&crop=0&crop=1&crop=1&id=rkLUs&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997554396-b1bdf738-0ed8-4550-a385-27a005c4e4e3.png#crop=0&crop=0&crop=1&crop=1&id=oMoK2&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997554689-76c7bf51-603b-46bf-b82d-e5cfa0915422.png#crop=0&crop=0&crop=1&crop=1&id=H0tit&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)





图 3.13 五通道卷积神经网络结构

      1. 五通道特征融合

在前面所提取到的微表情特征来自五个不同的通道，且提取到的特征各自包含的信息也各不相同，要在后面完成微表情识别分类，需要对 5 个通道的微表情特征进行融合形成一个包含微表情各种特征信息的新特征。本文使用级联
（concatenation）融合算法来完成特征融合的操作，为了将特征融合过程描述得更加具体，这里假设利用双通道来完成微表情特征提取，将双通道提取到的特征串联在一起进一步完成特征融合，该过程的函数表达式为：
_y	_ _xa_

   - _xb_

(3.52)

_i_, _j _,_d	i_, _j _,_d _ _D	i_, _j _,_d_

式（3.52）中， _xa _和 _xb _分别表示有相同维度的两个特征向量，融合后的特征用向

36

第三章 五通道卷积神经网络与胶囊网络的建立
量 _y _来表示，其中 y R_W__H_2_D _，_yb _ _RW__H__D _，_xa _ _RW__H__D _，1<i<H ，1<j<w ，1<d<D 。特征向量的宽和高分别是_W _和 _H _，特征向量的维度用 _D _表示。
作为五通道神经网络，前面提到开始帧和顶帧的水平光流、垂直光流作为其中两个通道的输入，开始帧和结束帧的水平光流、垂直光流作为另外两个通道的输入，剩下的一个通道输入顶帧的灰度图像。这里五个通道提取到的微表情特征信息分别用 C1、C2、C3、C4 以及 C5 来表示，那么将五个特征进入级联融合之后可以得到：
_C _ _C_1  _C_2  _C_3  _C_4  _C_5
(3.53)

上式中 表示对特征之间进行级联，C 表示对五个通道的输出特征进行融合之后得到的新特征向量。

      1. 五通道卷积神经网络和胶囊网络模型

胶囊网络在描述图像中目标之间的相对关系时具有良好的优势，将来自五个通道的特征融合后输入到胶囊网络模型中进一步对微表情特征增强来完成识别分类任务，起到增强特征提高识别精度的作用。在五通道卷积神经网络中加入胶囊网络后的结构如图 3.14 所示。这里特征融合之后再添加了两个卷积层可以进一步压缩特征信息提取出更具有代表性的特征。
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997555155-a3aad9ff-6ef3-41cf-ae27-27acab742474.png#crop=0&crop=0&crop=1&crop=1&id=Nz3Tf&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)Input 224*224
Conv1
Pool1
Conv2
Pool2
Conv3
Pool3






On_Ap_V
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997555653-e4d238c1-2c08-4c74-b5ff-83786f1586b7.png#crop=0&crop=0&crop=1&crop=1&id=a7ff7&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997556181-24801d50-86f5-47bc-81b3-1dd608f2904f.png#crop=0&crop=0&crop=1&crop=1&id=I4qzn&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997556694-ae51eddf-76cf-4d30-af4e-a324ba6a28da.png#crop=0&crop=0&crop=1&crop=1&id=JRuzG&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
![](https://cdn.nlark.com/yuque/0/2022/jpeg/22897659/1670997557213-024bcd03-1d4c-4000-a6cc-be4cd9d54a53.jpeg#crop=0&crop=0&crop=1&crop=1&id=qM8iq&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
On_Ap_H
![](https://cdn.nlark.com/yuque/0/2022/jpeg/22897659/1670997557662-bef5ba45-e197-40e4-9b1c-0ebef4f37a3d.jpeg#crop=0&crop=0&crop=1&crop=1&id=NzDrQ&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
Apex_Gray
![](https://cdn.nlark.com/yuque/0/2022/jpeg/22897659/1670997557964-6e7777fe-54fd-448f-99d5-f3475536e207.jpeg#crop=0&crop=0&crop=1&crop=1&id=r2Yff&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
Ap_Of_V
![](https://cdn.nlark.com/yuque/0/2022/jpeg/22897659/1670997558295-cd99725f-360c-4a5f-a8e4-941289016280.jpeg#crop=0&crop=0&crop=1&crop=1&id=BzpxA&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
Ap_Of_H




16	16


5
5


1
1










32	32	64

3
3

1
1

	



ME-PrimaryCaps












5 Labels

3
3
1
1
64
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997558768-eee28e06-50a1-43a7-b556-07d2280f2661.png#crop=0&crop=0&crop=1&crop=1&id=HJ0jk&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997559334-328b08d0-a06f-4b01-8440-ab1ab8fcf675.png#crop=0&crop=0&crop=1&crop=1&id=X3NS2&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997560319-c6431e78-f4d8-433a-b4d6-72a834be67b6.png#crop=0&crop=0&crop=1&crop=1&id=AlV6r&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997560827-65c1b65f-ef5b-43ba-942d-c92d1918fa91.png#crop=0&crop=0&crop=1&crop=1&id=SIlPl&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997561304-dacf377a-c1bd-4415-b8d3-6e135babd136.png#crop=0&crop=0&crop=1&crop=1&id=nq8x8&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
Conv4	Conv5
ME-Caps
3
3
3
3
1024	1024
16
ME-labels

18


...



32
Concat


图 3.14 五通道卷积神经网络和胶囊网络结构图

37
在图 3.14 中，由胶囊网络模型和五通道卷积神经网络共同来完成微表情识别任务。图中每个通道分别由三个卷积层和三个池化层且各通道之间具有相同的参数来共同组成。其每个卷积层后的卷积核数量分别是 16、32 和 64，其中第一个卷积核的大小是 5×5，剩下的两个卷积核的大小都是 3×3。而所有的池化层中的核大小都是 1×1，网络的详细信息见表 3.2，其中 K 表示核大小，P 表示图像填充数，S 表示核的步长。对于接入的胶囊网络，ME-PrimaryCaps 模块表示微表情初级胶囊，ME-Caps 模块表示微表情高级胶囊，ME-Labels 模块表示分类器预测微表情的输出类别。

|  | 表 3.2 | 五通道卷积神经网络参数信息 |  |
| --- | --- | --- | --- |
| Layer |  | Kernel Parameter | Output Size |
| Data |  |  | 1×224×224 |
| Conv 1 |  | K=5×5 ，P=1，S=2 | 16×112×112 |
| Pool 1 |  | K=1×1 ，P=0，S=1 | 16×112×112 |
| Conv 2 |  | K=3×3 ，P=1，S=2 | 32×56×56 |
| Pool 2 |  | K=1×1 ，P=0，S=1 | 32×56×56 |
| Conv 3 |  | K=3×3 ，P=1，S=2 | 64×28×28 |
| Pool 3 |  | K=1×1 ，P=0，S=1 | 64×28×28 |
| Conv 4 |  | K=3×3 ，P=1，S=2 | 1024×14×14 |
| Conv 5 |  | K=3×3 ，P=1，S=2 | 1024×14×14 |





















38

第四章 实验测试与结果分析
### 第四章 实验测试与结果分析
本章将对已经预处理好的微表情数据集在五通道卷积神经网络与胶囊网络模型进行实验测试和结果分析。首先使用开始帧和顶帧之间、顶帧和结束帧之间的光流信息分别加上顶帧的灰度图像作为三通道卷积神经网络的输入，对比开始帧、顶帧、结束帧之间的光流信息加上顶帧的灰度图像作为五通道卷积神经网络的输入。此外，在五通道卷积神经网络后面接入的胶囊网络改为全连接层进行比较，得出胶囊网络在对特征融合后分类识别更具有优势，进一步说明了本文提出的方法的有效性。同时与典型的微表情识别方法进行了结果对比，实验表明本文提出的方法能够有效改善微表情识别精度。
#### 0.11 实验设置
在实验中，为了验证方法更具有泛化性能，使用留一被试交叉验证法在 CASME Ⅱ和 SAMM 两个数据集上进行验证。CASME Ⅱ共包含 26 个被试 255 个样本，但有的被试在某种微表情样本中没有且进行的是五分类实验，因此实验中用到的总样本数为 246。同样，在 SAMM 中选取了样本较多的微表情数据共 27个被试 136 个样本。如表 4.1 所示神经网络的训练参数，对于留一被试法交叉验证法，即网络每次在训练过程中选取一个被试的样本数据作为测试集，剩余的作为训练集，实验最终将预测准确的某个样本并且将该预测结果记录到总准确率中。实验对所有微表情样本以及在求取光流信息后的光流图像大小统一设置为 224×224 。

|  | 表 4.1 | 神经网络训练参数设置 |  |
| --- | --- | --- | --- |
| 名称 |  |  | 参数大小 |
| 迭代次数 |  |  | 700 |
| 学习率 |  |  | 0.0001 |
| Batch-size
优化器 激活函数 |  |  | 8
Adam Relu |

#### 0.12 实验结果及分析
如 3.4 节中所介绍的那样，建立五通道卷积神经网络和胶囊网络后，在网络模型训练和测试时，对提取到的微表情光流信息作为五通道卷积神经网络的输入信号，网络训练好后通过留一被试交叉验证的方法对网络模型的识别效果进行评估，这里对微表情光流图像的名称进行规范如表 4.2 所示。为了说明本文方法对

39

微表情识别精度有较大的改善，本文与其它的研究方法进行了对比。
表 4.2 微表情及光流信息名称说明
名称	说明

Apex_Gray	微表情顶帧灰度图即 Apex_Gray
On_Ap_H	微表情开始帧和顶帧之间的水平光流
On_Ap_V	微表情开始帧和顶帧之间的垂直光流
Ap_Of_H	微表情顶帧和结束帧之间的水平光流
Ap_Of_V	微表情顶帧和结束帧之间的垂直光流

      1. 顶帧作为输入对比有无胶囊网络

当仅仅使用微表情顶帧灰度图来进行微表情识别任务时，我们设计了单通道卷积神经网络来提取微表情特征。前面的网络有相同的配置，每个卷积核的大小及数量都是相同的并且将后面分别接入全连接层与胶囊网络模型来进行微表情分类识别对比，该单通道卷积神经网络结构具有三个卷积核结构如图 4.1 所示。
Input 224*224
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997561843-53b31ccb-3771-46f2-9fd3-e50faaf9cbf8.png#crop=0&crop=0&crop=1&crop=1&id=XzeC8&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)

3
3
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997562315-7cf73723-8e83-482f-9548-41752c92bcf5.png#crop=0&crop=0&crop=1&crop=1&id=AnH4r&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)

1
1
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997562610-34cb57da-83ee-4e2d-82b6-82611056f223.png#crop=0&crop=0&crop=1&crop=1&id=oQmuq&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)

3
3
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997562948-8ce61cba-243a-419c-87e8-5281ce441708.png#crop=0&crop=0&crop=1&crop=1&id=RQaYJ&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)

1
1
Conv1
Pool1
Conv2
Pool2
Conv3
Pool3 FC


![](https://cdn.nlark.com/yuque/0/2022/jpeg/22897659/1670997563452-754c41f0-8435-4d2a-b506-ce4c431347a0.jpeg#crop=0&crop=0&crop=1&crop=1&id=sqtQd&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)

Apex_Gray
16	16	32	32	64	64
(a)
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997563786-63c6ab9e-bb0a-4667-b6c2-2a53bfadc830.png#crop=0&crop=0&crop=1&crop=1&id=Nb5Yk&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)


5
5
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997564145-b2051edd-7292-46a1-8513-d4ef5c94f79e.png#crop=0&crop=0&crop=1&crop=1&id=mJuBo&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)


1
1



Input
![](https://cdn.nlark.com/yuque/0/2022/jpeg/22897659/1670997564493-7f4f604c-527d-49ef-8717-d657a9ea0c30.jpeg#crop=0&crop=0&crop=1&crop=1&id=JLSo5&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)224*224

|  |
| --- |
|  |
|  |
|  |
|  |


Conv1
Pool1
Conv2
Pool2
Conv3
Pool3
ME-PrimaryCaps
ME-Caps






Apex_Gray




16	16
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997564883-291dba77-dc92-4225-ac75-0aa7affea2b4.png#crop=0&crop=0&crop=1&crop=1&id=fRwwM&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)


5
5
1
1
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997565186-261f3322-40d5-424c-a715-0f03e18fae37.png#crop=0&crop=0&crop=1&crop=1&id=XIX9e&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)


1
1
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997565550-127cf5a9-2d33-4b0d-9886-43e373c26b82.png#crop=0&crop=0&crop=1&crop=1&id=Ap1hy&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)

3
3
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997565928-0705fe0d-de64-401b-b582-14aa51f22850.png#crop=0&crop=0&crop=1&crop=1&id=JlEir&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997566265-b4733d25-f3f2-4483-96ac-57b66c72446c.png#crop=0&crop=0&crop=1&crop=1&id=CT7wZ&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)

3
3


32	32	64



64	16

         1. 


18


...


32
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997566625-071a8980-fd22-498e-a3c6-853e3c15742c.png#crop=0&crop=0&crop=1&crop=1&id=JvsoR&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)

1
1


图 4.1 单通道网络结构图: (a) 接入全连接层;(b) 接入胶囊网络
当微表情顶帧作为单通道网络的输入时，仅对后面使用全连接层或者胶囊网络进行识别结果的对比。实验在 CASME Ⅱ和SAMM 两个数据集上完成使用胶囊网络的识别精度为 55.28%和 47.2%分别高于使用全连接层的 51.26%和 42.64%，其识别结果见表 4.3。结果表明将后面的全连接层改为胶囊网络能进一步提取微表情的高水平特征，从而提高识别率。


40

第四章 实验测试与结果分析
表 4.3 单通道卷积神经网络的全连接和胶囊网络的结果对比

识别率
方法
CASME Ⅱ	SAMM


全连接

顶帧灰度图
51.62%	42.64%

胶囊网络	55.28%	47.20%

      1. 顶帧结合开始帧或结束帧在卷积神经网络中的比较

微表情顶帧中包含微表情特征信息较多，虽然微表情出现在面部肌肉中运动的持续时间极短约 1/25 秒-1/2 秒，但依然可以把微表情的出现看作一个随时间连续运动的过程，即运动开始、运动幅度峰值、运动结束，那么从微表情运动开始到运动幅度峰值，或者运动幅度峰值到微表情运动结束中哪一段区间对微表情的识别更有效？我们使用了带有胶囊网络的三通道卷积神经网络对微表情运动开始到运动峰值、微表情运动峰值到运动结束的结果进行了对比。三通道网络中前面所有的卷积层各类参数均有相同配置，网络结构如图 4.2 所示，识别结果如表 4.4。
Input 224*224
Conv1
Pool1
Conv2
Pool2
Conv3
Pool3
ME-PrimaryCaps

![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997567015-0f991e00-eff0-456b-97e6-3774eb7de9b2.png#crop=0&crop=0&crop=1&crop=1&id=jcGa0&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)

On_Ap_V/ Ap_Of_V
![](https://cdn.nlark.com/yuque/0/2022/jpeg/22897659/1670997567292-cc3a4459-5046-4f7d-8045-58a2a0e9610c.jpeg#crop=0&crop=0&crop=1&crop=1&id=EnTHO&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
Apex_Gray
![](https://cdn.nlark.com/yuque/0/2022/jpeg/22897659/1670997567634-c54b516c-9a88-4fa6-95be-8f5c646f6e7e.jpeg#crop=0&crop=0&crop=1&crop=1&id=NZqXP&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
On_Ap_H/ Ap_On_H

16	16
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997568050-6a5ba904-8931-4dcb-986e-6003198cb42a.png#crop=0&crop=0&crop=1&crop=1&id=UN8KY&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)


5
5
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997568330-fdf9abd6-46ac-41b3-bec9-a0c4de4a51ea.png#crop=0&crop=0&crop=1&crop=1&id=dn7O0&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)


1
1
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997568711-26426edc-7402-4a76-bf92-bb10b9946992.png#crop=0&crop=0&crop=1&crop=1&id=kIeta&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)

![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997569106-5966a921-ec62-4427-bfd8-6c8edd6fb95e.png#crop=0&crop=0&crop=1&crop=1&id=Ue8pH&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997569396-c079d137-6a3b-4709-a467-fc693e847226.png#crop=0&crop=0&crop=1&crop=1&id=wIwcY&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)


32	32
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997569824-2b1bd2d9-ad6d-49cf-b2b9-d0ff1afee487.png#crop=0&crop=0&crop=1&crop=1&id=Zp4tq&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)

3
3
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997570367-13741964-dc0d-4966-91ac-5067ba8925e8.png#crop=0&crop=0&crop=1&crop=1&id=M8NV4&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)

1
1
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997570882-90356864-182c-4333-8333-3b2046322665.png#crop=0&crop=0&crop=1&crop=1&id=Yf1AX&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)

![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997571403-4b8b03a5-aefc-4e66-933c-fb585cb4e094.png#crop=0&crop=0&crop=1&crop=1&id=XOBFS&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997571852-fee9bf0b-f659-49c8-b6ca-be603bdb728e.png#crop=0&crop=0&crop=1&crop=1&id=jObzw&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)


64	64
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997572377-18291889-c213-4163-8d7d-bf1b36f87384.png#crop=0&crop=0&crop=1&crop=1&id=rfRtJ&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)

3
3
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997572946-ea246b3a-8b2a-40f9-acc5-1cf84761f191.png#crop=0&crop=0&crop=1&crop=1&id=dtQRc&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)

1
1


 
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997573415-d341e958-64cd-4e4f-b4ac-69afff55b198.png#crop=0&crop=0&crop=1&crop=1&id=zjYnq&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997573945-42ad8add-ee33-4c49-9f83-c208e72cffe1.png#crop=0&crop=0&crop=1&crop=1&id=g3IGe&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)


 
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997574494-9269bbdf-8dcf-48e0-baaa-2cb229d9cb72.png#crop=0&crop=0&crop=1&crop=1&id=Pr5CB&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997574985-75007ce5-96de-44d4-863d-b8ff4fab5f59.png#crop=0&crop=0&crop=1&crop=1&id=TNBib&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)


![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997575422-3dbdb412-b2f2-4b9f-83de-f0d8f9c21226.png#crop=0&crop=0&crop=1&crop=1&id=rmazd&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
Conv4 Conv5

3	3
3	3


1024
1024

ME-labels
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997575678-7807d567-8bf8-4f4c-9c63-591ec7fe4415.png#crop=0&crop=0&crop=1&crop=1&id=gGiLr&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997575999-37f7e339-bf67-412c-b6fb-e0566d01ada4.png#crop=0&crop=0&crop=1&crop=1&id=ECuim&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997576253-10975bcb-a405-4c49-a8e4-eafed569e65d.png#crop=0&crop=0&crop=1&crop=1&id=qTlHp&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997576512-04e72335-f041-4da7-a246-ed1009726105.png#crop=0&crop=0&crop=1&crop=1&id=lEkIk&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997576750-ec940935-1e24-4fd5-9d6b-c4f86a354880.png#crop=0&crop=0&crop=1&crop=1&id=Zealh&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)



5 Labels


ME-Caps

18


...



32




16


图 4.2 三通道卷积神经网络结构图
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997577109-edcb8ac6-19ce-4b0c-846c-13958fdd2778.png#crop=0&crop=0&crop=1&crop=1&id=b6Z1G&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997577672-c1b04085-3ded-439b-8f40-84a188b2cd2c.png#crop=0&crop=0&crop=1&crop=1&id=vUsv9&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
在表 4.4 中，顶帧结合开始帧或结束帧作为三通道卷积神经网络的识别结果在 CASME Ⅱ和 SAMM 两个数据库上进行试验，当顶帧和结束帧之间的水平光流和垂直光流作为卷积神经网络的输入时，识别率分别为 60.97%和 52.2%，而开始帧与顶帧之间的水平光流和垂直光流作为卷积神经网络的输入其识别率分别为 62.60%和 53.67%，相比较而言后者的识别率有明显的提高分别为 1.63%和 1.47%。
41

该结果表明微表情顶帧和结束帧之间的光流信息存在一定的相关性，且对微表情识别率存在一定的影响。
为更进一步探讨微表情开始帧、顶帧以及结束帧之间的对微表情识别的影响，实验使用图 3.16 的网络结构，将微表情开始帧、顶帧以及结束帧之间的光流图像和顶帧灰度图像作为网络的输入，结合胶囊网络模型来提取融合后的高水平微表情特征。实验结果如下表 4.4 所示，与顶帧和结束帧作为三通道卷积神经网络的输入相比较而言，当开始帧、顶帧以及结束帧各自之间的光流信息作为五通道卷积神经网络的输入时，其识别率达到了 64.63%和 55.88%，比前者提高了 2.03 和
2.21 个百分点。结果表明对于开始帧和顶帧之间的相关性特征加上顶帧和结束帧之间的相关性特征更具有代表性，对提升微表情识别精度有较大的影响。
表 4.4 顶帧结合开始帧或结束帧在三通道卷积神经网络中的结果对比
识别率
方法
CASME Ⅱ	SAMM



三通道卷积神经网络
开始帧与顶帧	62.60%	53.67%
顶帧与结束帧	60.97%	52.20%

五通道卷积神经网络	开始帧与顶帧及结束帧	64.63%	55.88%

      1. 五通道卷积神经网络与胶囊网络模型和对比算法的识别结果比较

本文使用五通道卷积神经网络与胶囊网络模型和典型的微表情识别算法进行比较，包括 STCLQP、MDMO、LBP-SIP 以及 LBP-TOP。用 SVM 算法进行分类且使用留一被试法进行交叉验证。

         1. STCLQP。2016 年由 Huang 等人提出的时空域完全的局部量化模式

（STCLQP），首先对微表情的符号、大小以及方向的三个分量特征进行矢量量化编码，随后进行特征融合，结合 SVM 完成分类。

         1. MDMO。2016 年 Liu 等人提出的主方向平均光流（MDMO），该算法先求得微表情的光流信息后再对微表情划分为 36 个人脸子区域最终再分别计算主方向上的平均光流。
         2. LBP-SIP。2014 年，Wang 等人提出带六个交点的局部二值模式算法

（LBP-SIP），该算法利用 X、Y、Z 正交面来计算微表情的时空模式，随后求得微表情的特征直方图并将所有特征合并到高斯金字塔中完成微表情分类。

         1. LBP-TOP。通过纹理特征提取描述算法，该方法在三个时空 X、Y、T上设定 XY 平面包含图像的纹理特征信息，YT 和 XT 分别包含了图片序列的时间变化和空间位置的变化。随后在 X、Y、Z 方向上分别计算 LBP 的特征值，再将这三个平面的特征串联起来完成特征分类。

42

第四章 实验测试与结果分析

         1. Residual Network。2018 年，Peng 等人提出在小样本数据集上使用迁移学习的方法，通过宏表情数据集预训练网络模型，随后在微表情数据集上微调模型参数，最后完成微表情分类。
         2. DSSN。2019 年，Khor 等人提出一个轻量级的双通道浅层网络，其形式是一对截断的 CNN 异构的输入特性[102]。对提取到的特征进行合并后再对微表情按成识别分类。

在表 4.5  中，展示了五通道卷积神经网络和胶囊网络模型与以上对比算法在 SAMM 和 CASME Ⅱ上的识别结果。对于所有对比的方法，我们使用留一被试交叉验证的方法来验证算法的性能。其中 LBP-TOP 的识别率低于LBP-TOP、MDMO和 STCLQP 的识别率。而本文提出的方法在上面所有方法中获得了最好的识别精度，在 CASME  Ⅱ和 SAMM 上的识别精度分别为 64.63%和 55.88%，结果表明本文利用微表情开始帧、顶帧以及结束帧结合五通道卷积神经网络和胶囊网络模型对于微表情识别精度改善有较大的作用。
表 4.5 五通道卷积神经网络结合胶囊网络模型和其它算法的结果对比

|  |  | 识别率 |  |
| --- | --- | --- | --- |
| 方法 | CASME Ⅱ |  | SAMM |
| LBP-TOP | 47.63% |  | 40.24% |
| LBP-SIP | 51.62% |  | 42.64% |
| MDMO | 55.28% |  | 47.20% |
| STCLQP | 58.23% |  | 52.31% |
| Residual Network | 62.20% |  | 45.6% |
| DSSN | 62.96% |  | 52.94% |
| 五通道卷积神经网络和胶囊网络模型 | **64.63%** |  | **55.88%** |


















43

![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997578295-7e74bdd6-e1b1-449d-bef4-645f47feb142.png#crop=0&crop=0&crop=1&crop=1&id=fBIyU&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997578700-bd1f20f4-869b-416f-b79a-d3e21053e6e4.png#crop=0&crop=0&crop=1&crop=1&id=vUkx9&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997579210-0ed05fdc-72e9-48e6-a2b3-8f3720cb78c3.png#crop=0&crop=0&crop=1&crop=1&id=PPslg&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997579665-2448e3c7-5e34-4302-a46e-3bd837fc4a30.png#crop=0&crop=0&crop=1&crop=1&id=lzIrc&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)




#### 0.13 论文总结
第五章 总结与展望
### 第五章 总结与展望


当人们受到一定程度的心理刺激时面部小部分区域会不由自主的运动进而产生微表情，微表情能有效揭示人们的心理情绪状态或自身的真情实感，使得微表情在谎言识别、医疗诊断以及社会安全等方面有着巨大的应用价值。但是，由于微表情运动只出现在面部小部分区域且持续时间极短、运动强度低等特点，使得现有方法难以从微表情中提取有效的微表情特征信息，此外微表情只有小部分面部肌肉运动且运动过程是连续的，因此对所有微表情视频帧提取特征更多的是冗余特征信息，从而在很大程度上给微表情识别带来了一定的困难。因此，本文提出一种基于五通道卷积神经网络和胶囊网络模型来完成微表情识别任务。该方法五通道卷积神经网络分别提取开始帧、顶帧以及结束帧之间的水平光流和垂直光流的低水平特征信息，随后将五个通道的特征信息进行融合后作为胶囊网络的输入，进一步提取高水平特征完成微表情识别分类任务，实验结果表明该方法对改善微表情识别分类精度十分有效。本文的工作可以主要有以下几个方面：

      1. 将微表情与神经网络相结合，提出了基于五通道卷积神经网络对微表情提取低水平特征信息的方法，同时针对现有数据集样本较少的问题，设计了只有三个卷积层的五通道卷积神经网络。
      2. 论文选择了微表情的开始帧、顶帧以及结束帧，包含微表情运动信息较多的帧来提取光流特征信息作为卷积神经网络其中四个通道的输入，另外一个通道输入微表情顶帧的灰度图像，进一步对微表情特征信息进行加强以此减少微表情特征的冗余信息，确保每个通道能提取到较丰富的微表情特征。
      3. 在五通道卷积神经网络后面加入胶囊网络模型，对融合后的特征进一步增强，从而自适应地学习微表情特征权重，使微表情的分类精度达到更好的效果。
#### 0.14 论文展望
近年来，微表情受到越来越多研究者的关注，改善其识别效果对于微表情的理论意义和实际意义有着重大的影响，同时提升微表情识别精度的方法也层出不穷。本文提出的五通道卷积神经网络与胶囊网络模型尽管在识别精度上有所提升，但识别结果距离在实际应用中还有提升的空间；此外，自动微表情分析系统包括微表情识别与检测两个部分，而在本文的分析研究中还没有涉及到微表情检测。
就目前微表情的研究进展而言，接下来可以从以下两个方面来对微表情相关的分析研究不断深化：
（1）关于微表情数据库。微表情数据库质量的好坏对微表情起着决定性作用，

45
就现有微表情公开数据库而言，存在微表情样本质量差、数量少以及样本数量不均衡等问题。因此，要使深度学习在微表情识别上增强其鲁棒性、泛化能力等问题，亟需建立样本平衡且充足的微表情数据库。
（2）微表情逐渐受到许多研究者的关注，利用深度学习的方法建立微表情自动分析识别系统也受到了青睐。但微表情识别首先第一步需要完成的是微表情检测工作，即在一段微表情序列中完成微表情定位后，通过训练深度学习模型来对定位样本进行分类识别。微表情检测与微表情识别密不可分，只有同时加强这两方面的研究以使微表情自动分析识别系统应用到真实的场景中。









































46

参考文献

### 参考文献

1. Mehrabian A. Communication without words. University of East London, 1968, 24(4): 1084-1085.
2. Uçar A, Demir Y, Güzeliş C. A new facial expression recognition based on curvelet transform and online sequential extreme learning machine initialized with spherical clustering. Neural Computing and Applications, 2016, 27(1): 131-142.
3. Corneanu, Ciprian, Adrian, et al. Survey on RGB, 3D, Thermal, and Multimodal Approaches for Facial Expression Recognition: History, Trends, and Affect-Related Applications. IEEE Transactions on Pattern Analysis & Machine Intelligence, 2016, 38(8): 1548-1568.
4. Zhao G, Huang X, Taini M, et al. Facial expression recognition from near-infrared videos. Image & Vision Computing, 2011, 29(9): 607-619.
5. Ekman P, Darwin. Deception and Facial Expression. 2003, 1000(1): 205-221.
6. Wang S J, Chen H L, Yan W J, et al. Face Recognition and Micro-expression Recognition Based on Discriminant Tensor Subspace Analysis Plus Extreme Learning Machine. NEURAL PROCESSING LETTERS, 2014, 39(1): 25-43.
7. Huang X, Zhao G, Hong X, et al. Spontaneous facial micro-expression analysis using Spatiotemporal Completed Local Quantized Patterns. Neurocomputing, 2016, 175(JAN.29PT.A): 564-578.
8. Xu F, Zhang J, Wang J Z. Microexpression Identification and Categorization Using a Facial Dynamics Map. IEEE Transactions on Affective Computing, 2017, 8(2): 254-267.
9. Haggard E A, Isaacs K S. Micromomentary facial expressions as indicators of ego mechanisms in psychotherapy. 1966.
10. Ekman P, Friesen W. Nonverbal leakage and cues to deception. Psychiatry-interpersonal and Biological Processes, 1969, 32(1): 88-106.
11. Ekman P. Micro Expressions Training Tool. Emotionsrevealed.com, 2003.
12. Frank, Ekman M G, Paul. The ability to detect deceit generalizes across different types of high-stake lies. Journal of Personality and Social Psychology, 1997.
13. Yan W J, Wu Q, Liang J, et al. How Fast are the Leaked Facial Expressions: The Duration of Micro-Expressions. Journal of Nonverbal Behavior, 2013, 37(4): 217-230.

47

1. Porter S, Brinke L T. Reading Between the Lies: Identifying Concealed and Falsified Emotions in Universal Facial Expressions. Psychological Science, 2008, 19(5): 508-514.
2. Ekman P. Lie Catching and Microexpressions. 2009.
3. Weinberger, Sharon. Airport security: Intent to deceive? Nature, 2010, 465(7297): 412-415.
4. Ekman P, Friesen W. The Facial Action Coding System. 1978.
5. Frank M G, Herbasz M, Sinuk K. I see how you feel: Training laypeople and professionals to recognize fleeting  emotions. The  Annual  Meeting  of the International Communication Association.Sheraton New York, New York City, 2009.
6. Liong S T, See J, Wong K S, et al. Automatic Micro-expression Recognition from Long Video Using a Single Spotted Apex. Asian Conference on Computer Vision, 2017.
7. Yong-Jin, Liu, Jin-Kai, et al. A Main Directional Mean Optical Flow Feature for Spontaneous Micro-Expression Recognition. IEEE Transactions on Affective Computing, 2016, 7(4): 299-310.
8. Krizhevsky A, Sutskever I, Hinton G E. ImageNet classification with deep convolutional neural networks. Communications of the Acm, 2017, 60(6): 84-90.
9. Sun Y, Liang D, Wang X, et al. DeepID3: Face Recognition with Very Deep Neural Networks. Computer Science, 2015.
10. Xiong W, Droppo J, Huang X, et al. The Microsoft 2016 Conversational Speech Recognition System. 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2017.
11. Wu Y, Schuster M, Chen Z, et al. Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. 2016.
12. Gibney E. What Google's winning Go algorithm will do next. Nature, 2016, 531(7594): 284.
13. Peng M, Wang C, Chen T, et al. Dual Temporal Scale Convolutional Neural Network for Micro-Expression Recognition. Frontiers in Psychology, 2017, 8(1): 1745.
14. Polikovsky S, Kameda Y, Ohta Y. Facial micro-expressions recognition using high speed camera and 3D-gradient descriptor. International Conference on

48

参考文献
Crime Detection & Prevention, 2010.

1. Wu Q, Shen X, Fu X. The machine knows what you are hiding: An automatic micro-expression recognition system. Lecture Notes in Computer Science, 2011, 6975(152-162).
2. Li X, Pfister T, Huang X, et al. A Spontaneous Micro-expression Database: Inducement, collection and baseline. Automatic Face and Gesture Recognition (FG), 2013 10th IEEE International Conference and Workshops on, 2013.
3. Yan W J, Wu Q, Liu Y J, et al. CASME database: A dataset of spontaneous micro-expressions collected from neutralized faces. IEEE International Conference & Workshops on Automatic Face & Gesture Recognition, 2013.
4. Yan W J, Li X, Wang S J, et al. CASME II: An Improved Spontaneous Micro-Expression Database and the Baseline Evaluation. Plos One, 2014, 9(1): e86041.
5. Davison A K, Lansley C, Costen N, et al. SAMM: A Spontaneous Micro-Facial Movement Dataset. IEEE Transactions on Affective Computing, 2018, 9(99): 116-129.
6. Pfister T, Li X, Zhao G, et al. Recognising spontaneous facial micro-expressions. International Conference on Computer Vision, 2012.
7. Wang Y, See J, Phan W, et al. LBP with Six Intersection Points: Reducing Redundant Information in LBP-TOP for Micro-expression Recognition. ACCV2014, 2015.
8. Huang X, Wang S-J, Zhao G, et al. Facial Micro-Expression Recognition Using Spatiotemporal Local Binary Pattern with Integral Projection. 2015, 2015.1-9.
9. Kamarol S K A, Meli N S, Jaward M H, et al. Spatio-temporal texture-based feature extraction for spontaneous facial expression recognition. Iapr International Conference on Machine Vision Applications, 2015. 467-470.
10. Park S Y, Lee S H, Ro Y M. Subtle Facial Expression Recognition Using Adaptive Magnification of Discriminative Facial Motion. Acm International Conference on Multimedia, 2015.
11. Duan X, Dai Q, Wang X, et al. Recognizing Spontaneous Micro-Expression from Eye Region. Neurocomputing, 2016, 217(DEC.12): 27-36.
12. Liong S, Phan R, See J, et al. Optical strain based recognition of subtle emotions. International Symposium on Intelligent Signal Processing and Communication Systems (ISPACS). Kuching. 2014 International Symposium

49


on IEEE, 2014, 180-184.


1. Liong S T, See J, Phan C W, et al. Subtle Expression Recognition Using Optical Strain Weighted Features. Springer International Publishing, 2014.
2. Wang S J, Yan W J, Zhao G, et al. Micro-Expression Recognition Using Robust Principal Component Analysis and Local Spatiotemporal Directional Features. Workshop at the European Conference on Computer Vision, 2014.
3. Zhang X, Tian Y, Guo Y, et al. Micro-Expression Recognition Based on Feature Combination of Optical Flow and LBP-TOP. Journal of Jilin University (Information Science Edition), 2015, 5(1): 004.
4. Le Ngo A C, See J,Raphael Phan C W. Sparsity in Dynamics of Spontaneous Subtle Emotion: Analysis & Application. IEEE Transactions on Affective Computing, 2016, 2016.1-1.
5. Liong S, See J, Wong K, et al. Less is More: Micro-expression Recognition from Video using Apex Frame. Signal Processing:Image Communication. 2016,
6. Wang S J, Yan W J, Li X, et al. Micro-expression Recognition Using Dynamic Textures on Tensor Independent Color Space. 2014 22nd International Conference on Pattern Recognition (ICPR), 2014.
7. Wang S J, Yan W J, Li X, et al. Micro-Expression Recognition Using Color Spaces. IEEE Transactions on Image Processing, 2015, 24(12): 6034-6047.
8. Su-Jing, Wang, Wen-Jing, et al. Sparse tensor canonical correlation analysis for micro-expression recognition. Neurocomputing, 2016, 214(1): 218-232.
9. Ben X, Peng Z, Rui Y, et al. Gait recognition and micro-expression recognition based on maximum margin projection with tensor representation. Neural Computing & Applications, 2015, 27(8): 1-18.
10. Peng M, Wu Z, Zhang Z, et al. From Macro to Micro Expression Recognition: Deep Learning on Small Datasets Using Transfer Learning. 2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018), 2018.
11. Wang C, Peng M, Bi T, et al. Micro-attention for micro-expression recognition. Neurocomputing, 2020, 410(0): 354-362.
12. Liong S T, Gan Y S, Yau W C, et al. OFF-ApexNet on Micro-expression Recognition System. 2018.
13. Wang, Su-Jing, Li, et al. Micro-expression recognition with small sample size by transferring long-term convolutional neural network. Neurocomputing, 2018.

50

参考文献

1. Liong S T, Gan Y S, Zheng D, et al. Evaluation of the Spatio-Temporal Features and GAN for Micro-Expression Recognition System. Journal of Signal Processing Systems, 2020, 92(1).
2. Song B, Li K, Zong Y, et al. Recognizing Spontaneous Micro-Expression Using a Three-Stream Convolutional Neural Network. IEEE Access, 2019, PP(99): 1-1.
3. Peng M, Wang C, Gao Y, et al. Recognizing Micro-expression in Video Clip with Adaptive Key-frame Mining. 2020.
4. Lei L, Li J, Chen T, et al. A Novel Graph-TCN with a Graph Structured Representation for Micro-expression Recognition. Proceedings of the 28th ACM International Conference on Multimedia (ACM MM 20), 2020.
5. Chen B, Zhang Z, Liu N, et al. Spatiotemporal Convolutional Neural Network with Convolutional Block Attention Module for Micro-Expression Recognition. Information (Switzerland), 2020, 11(8): 380.
6. Moilanen A, Zhao G,Pietikainen M. Spotting Rapid Facial Movements from Videos Using Appearance-Based Feature Difference Analysis. 2014, 2014.1722-1727.
7. Li X B, Xiao P, et al. Towards Reading Hidden Emotions: A Comparative Study of Spontaneous Micro-Expression Spotting and Recognition Methods. IEEE Transactions on Affective Computing, 2017, PP(99): 1-1.
8. King D E. Dlib-ml: A Machine Learning Toolkit. The Journal of Machine Learning Research, 2009.
9. Shreve M, Godavarthy S, Manohar V, et al. Towards macro- and micro-expression spotting in video using strain patterns. 2009.
10. Wang S J, Wu S, Fu X. A Main Directional Maximal Difference Analysis for Spotting Micro-expressions. Asian Conference on Computer Vision, 2016.
11. Peng X, Wang L, Wang X, et al. Bag of Visual Words and Fusion Methods for Action Recognition: Comprehensive Study and Good Practice. Computer Vision & Image Understanding, 2016, 150(Sep.): 109-125.
12. Horn B K P, Schunck B G. Determining Optical Flow. Artificial Intelligence, 1981, 17(1-3): 185-203.
13. Sermanet P, Eigen D, Zhang X, et al. OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks. arXiv preprint arXiv:1312.6229, 2013.

51

1. Szegedy C, Toshev A, Erhan D. Deep neural networks for object detection. Advances in Neural Information Processing Systems, 2013, 26(1): 2553-2561.
2. Krizhevsky A, Sutskever I, Hinton G E. ImageNet classification with deep convolutional neural networks. Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1. Lake Tahoe, Nevada. Curran Associates Inc., 2012, 1097–1105.
3. Maggiori E, Tarabalka Y, Charpiat G, et al. Convolutional Neural Networks for Large-Scale Remote Sensing Image Classification. IEEE Transactions on Geoscience & Remote Sensing, 2016, 55(2): 645-657.
4. Liu C. Beyond pixels : exploring new representations and applications for motion analysis. Electrical Engineering and Computer Science. Massachusetts Institute of Technology. 2009.
5. Hinton G E, Salakhutdinov R R. Supporting Online Material for "Reducing the Dimensionality of Data with Neural Networks". Methods, 2006, 313.5786(2006): 504-507.
6. Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.
7. Abdel-Hamid O, Mohamed A-r, Jiang H, et al. Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition. 2012 IEEE international conference on Acoustics, speech and signal processing (ICASSP), IEEE, 2012. 4277-4280.
8. Abdelhamid O, Deng L, Yu D. Exploring Convolutional Neural Network Structures and Optimization Techniques for Speech Recognition. Interspeech, 2013. 1173-1175.
9. Cho K, Van Merrienboer B, Gulcehre C, et al. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. arXiv preprint arXiv:1406.1078, 2014.
10. Mcculloch W S, Pitts W. A Logical Calculus of the Ideas Immanent in Nervous Activity. biol math biophys, 1943, 5(4): 115-133.
11. Attneave F, Hebb D O. The Organization of Behavior; A Neuropsychological Theory. American Journal of Psychology, 1950, 63(4): 633.
12. Rosenblatt, F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological Review, 1958, 65(0): 386-408.
13. Mullin A A, Rosenblatt F. Principles of Neurodynamics. Perceptrons and the

52

参考文献
Theory of Brain Mechanisms, 1962, 70(5): 586.

1. Marvin M, Seymour A P. Perceptrons. 1969.
2. Hopfield J J. Neural networks and physical systems with emergent collective computational abilities. 1999. 7–19.
3. Rumelhart D E, Hinton G E, Williams R J. Learning Representations by Back Propagating Errors. Nature, 1986, 323(6088): 533-536.
4. Hecht-Nielsen R. Neurocomputer applications. Neural computers. Springer BerlinHeidelberg, 1989, 1989.445-453.
5. Quinlan R J. Induction of Decision Trees. Machine Learning, 1986, 1(1): 81-106.
6. Cortes C, Vapnik V. Support-Vector Networks. Machine Learning, 1995, 20(3): 273-297.
7. Breiman L. Random forests. Machine Learning, 2001, 45(1): 5-32.
8. Hinton G E, Salakhutdinov R R. Supporting Online Material for "Reducing the Dimensionality of Data with Neural Networks". Methods, 2006.
9. HINTON G E. Learning and relearning in Boltzmann machines. Parallel Distributed Processing, 1986, 1(282-317): 2.
10. Hinton G E, Osindero S, Teh Y W. A fast learning algorithm for deep belief nets. Neural Computation, 2006, 18(7): 1527-1554.
11. Szegedy C, Liu W, Jia Y, et al. Going Deeper with Convolutions. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014, 2014.1-9.
12. He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, 2016.770-778.
13. Waibel A, Hanazawa T, Hinton G, et al. Phoneme recognition using time-delay neural networks. IEEE Transactions on Acoustics Speech & Signal Processing, 2002, 37(3): 328-339.
14. Zhang W. Shift-invariant pattern recognition neural network and its optical architecture. Proceedings of annual conference of the Japan Society of Applied Physics, 1988.
15. Lecun Y, Boser B, Denker J, et al. Backpropagation Applied to Handwritten Zip Code Recognition. Neural Computation, 2014, 1(4): 541-551.
16. Lecun Y, Bottou L. Gradient-based learning applied to document recognition.

53
Proceedings of the IEEE, 1998, 86(11): 2278-2324.

1. Hinton G E, Srivastava N, Krizhevsky A, et al. Improving neural networks by preventing co-adaptation of feature detectors. Computer Science, 2012, 3(4): 212-223.
2. Sabour S, Frosst N, Hinton G E. Dynamic routing between capsules. arXiv preprint arXiv:1710.09829, 2017.
3. 张玉宏. 深度学习之美. 北京: 电子工业出版社, 2018, 583-590.
4. Haggard E A, Isaacs K S. Micromomentary facial expressions as indicators of ego mechanisms in psychotherapy. 1966. 154-165.
5. Peng M, Wang C, Chen T, et al. Nirfacenet: A convolutional neural network for near-infrared face identification. Information, 2016, 7(4): 61.
6. Wu Z, Peng M, Chen T. Thermal face recognition using convolutional neural network. 2016 International Conference on Optoelectronics and Image Processing (ICOIP), IEEE, 2016. 6-9.
7. Wu Z, Chen T, Chen Y, et al. NIRExpNet: Three-stream 3D convolutional neural network for near infrared facial expression recognition. Applied Sciences, 2017, 7(11): 1184.
8. Khor H Q, See J, Liong S T, et al. Dual-stream Shallow Networks for Facial Micro-expression Recognition. 2019 IEEE International Conference on Image Processing (ICIP). IEEE, 2019.
























54

致 谢

### 致 谢
师恩如海，长润我心；荏苒三载，倾我至诚！
本论文是在我的导师陈通教授指导下完成，从论文的选题到撰写完成，无不凝聚着导师的汗水和心血，恩师是一位谦和慈祥的长辈，在研究的开展过程中，经常给予深刻的讨论、独到的见解，问题都得以解决。您在我三年的学术与生活中给予的悉心指导和诸多关怀给我留下了深刻的印象。您渊博的专业知识、与时俱进的科研作风、立足科研切实解决社会问题的忘我奉献精神教会我在学术研究上应求真务实、知难而进，您严谨的治学态度、授人以渔的教学方法、严于律己、宽以待人的崇高风范、朴实无华、平易近人的人格魅力教会我人生道路上应戒骄戒躁、砥砺前行。恩师优秀的道德品质深深地烙印在我的脑海里，时时刻刻影响着我。在此向恩师致以衷心的感谢和深深的敬意。
衷心感谢刘光远教授带领的情感计算实验室下的各位老师，感谢你们在学术讨论上所作的精彩报告，感谢你们为我创造良好的科研实验环境进行学术研究，让我在情感计算这个方向上深受启迪、受益匪浅，使我在做课题研究的道路上行稳致远。
衷心感谢情感计算实验室的张志豪、陈莹、于业达、姚连升、李波、陈菊香和周巨等师兄师姐在学术上的解惑，感谢同年级的国雷、谭阳、刘欣雨、陈柏宇、王会平、熊荣龙、孔凡梦、杨雪红等同门好友，感谢你们在我遇到困难时给予的帮助。感谢 25 教 503，22 教 206 的各位师弟师妹以及寝室的同学，让我在日常忙碌的研究生活中也体会到了很多温暖，在此表示由衷的感谢。
特别地，还要由衷的感谢我的家人，对我学业的大力支持，对我倾注的心血和期望，这些永远是我的精神支柱，在此表示深深的谢意。
毕业季悄然而至，往事已不再有，离校时光的棘轮不停转动。回想那是 2018
年的夏天，心中满怀憧憬与期待的我第一次踏入理想的校园，而在 2021 年疫情阴霾散去的时刻却是该道离别、开启人生新征程的时候了，这段旅程是那么远，却又那么近。至此毕业论文的完成也已进入尾声，在硕士生涯的学术与生活中，期间始终都有老师、同学、家人以及朋友给予无私的帮助，在这里请接受我最诚挚的谢意。
最后，衷心地感谢各位在百忙中抽出时间评阅论文和参与答辩的专家、教授！


刘念
二零二一年三月 于重庆西南大学


55

![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997580175-30c50345-72ce-4174-a9bf-45fc2febc3bd.png#crop=0&crop=0&crop=1&crop=1&id=hFz1X&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997580385-6cadbcd5-4550-4b53-a2c7-8897358e8885.png#crop=0&crop=0&crop=1&crop=1&id=gRJY7&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997580717-abe0f4ff-b410-4b56-9818-de6a33922b5a.png#crop=0&crop=0&crop=1&crop=1&id=utgm7&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997580953-81ec0e18-8d3b-4964-8fe7-7b825369a929.png#crop=0&crop=0&crop=1&crop=1&id=ULVbI&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)

攻读硕士期间已发表的学术论文
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997581240-a986e810-f831-4529-bbf8-c88210a33861.png#crop=0&crop=0&crop=1&crop=1&id=uYuuj&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997581482-a7c37045-771b-4e37-8125-bbd06c1b9379.png#crop=0&crop=0&crop=1&crop=1&id=wFm3N&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
### 攻读硕士期间已发表的学术论文

   1. **Nian Liu**, et al. Offset or Onset Frame: A Multi-Stream Convolution Neural Network with CapsuleNet Module for Micro-expression Recognition. 2020 5th International Conference on Intelligent Informatics and Biomedical Sciences (ICIIBMS), 2020 [C]. IEEE.
   2. Boyu Chen, Zhihao Zhang, **Nian Liu**, et al. Spatiotemporal Convolutional Neural Network with Convolutional Block Attention Module for Micro-Expression Recognition. Information (Switzerland), 2020, 11(8): 380.
   3. Yuye Da, Liuxin Yu, **Liu Nian**, et al. SWU-NIRPV: A near-infrared pose variation face database and pose-invariant face recognition. Journal of Electronic Imaging, 2021,30(2):023018.







































57

![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997581748-b762e60d-2682-4a2c-b6b1-77eec466f2cd.png#crop=0&crop=0&crop=1&crop=1&id=rVaan&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)


![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997581970-01511444-3328-4528-ab86-3bbeaa36a9c4.png#crop=0&crop=0&crop=1&crop=1&id=DElzb&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997582294-1dfa559b-230a-4819-b408-14a10aafd1c8.png#crop=0&crop=0&crop=1&crop=1&id=dyZCc&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997582540-063cec70-7b4a-484b-bf80-cdcae8655977.png#crop=0&crop=0&crop=1&crop=1&id=ixWWk&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997582806-38bb0e4e-6571-4cca-ac9d-76faf5fe6f6a.png#crop=0&crop=0&crop=1&crop=1&id=zNnmh&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)

攻读硕士期间参加的科研项目
![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997583046-0d214a92-e527-4344-8bf1-ddcdad0e6cc1.png#crop=0&crop=0&crop=1&crop=1&id=vE7rO&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997583341-405e1fa8-950b-45cb-862f-3f66532320a8.png#crop=0&crop=0&crop=1&crop=1&id=wQqkR&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)

### 攻读硕士期间参加的科研项目

1. 国家重点研发计划子课题，XXXXXXX，2019-2021，参与。
2. 国家重点研发计划子课题，XXXXXXX，2018-2022，参与。















































59

![](https://cdn.nlark.com/yuque/0/2022/png/22897659/1670997583589-a281be60-6436-4c87-99d9-9bcb5c172a07.png#crop=0&crop=0&crop=1&crop=1&id=gfFKt&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
